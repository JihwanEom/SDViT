Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0,"EMA":"True","CutMix":"True"}
	hparams_seed: 0
	output_dir: ./domainbed/OfficeHome/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/EMA_CutMix/525d2f4edcd6e722e42b33acbb92b7f9
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 1526041754
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 1
	uda_holdout_fraction: 0
HParams:
	CutMix: True
	EMA: True
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_env: [3]
	weight_decay: 0.0
device: cuda
/local_ssd2/jeom/SDViT/domainbed/algorithms.py:196: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)
  t_param.data.mul_(self.ema_decay).add_(1 - self.ema_decay, s_param.data)
Best model upto now
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.0231719876  0.0288659794  0.0206185567  0.0206185567  0.0194256757  0.0157835400  0.0249569707  0.0252583238  0.0000000000  4.3099484444  7.8693671227  0             1.4584777355 
Best model upto now
0.2075180227  0.2061855670  0.1537800687  0.1500572738  0.2139639640  0.1972942503  0.1953528399  0.1963260620  4.9433573635  1.8755703497  8.1164951324  300           0.3878214741 
Best model upto now
0.6426364573  0.5525773196  0.5650057274  0.4936998855  0.7263513514  0.6696730552  0.6026965003  0.6130884041  9.8867147271  0.9832061542  8.1189212799  600           0.3887897221 
Best model upto now
0.8650875386  0.7731958763  0.7938144330  0.7044673540  0.9023085586  0.8331454340  0.7659208262  0.7680826636  14.830072090  0.8596045878  8.1189212799  900           0.6729079072 
Best model upto now
0.9577754892  0.8309278351  0.8920389462  0.7812142039  0.9541103604  0.8838782413  0.8123924269  0.8151549943  19.773429454  0.7734185981  8.1193943024  1200          0.7109669773 
Best model upto now
0.9819773429  0.8288659794  0.9384306987  0.8132875143  0.9743806306  0.9109357384  0.8333333333  0.8404133180  24.716786817  0.6673157884  8.1193943024  1500          0.7315163215 
Best model upto now
0.9886714727  0.8474226804  0.9642038946  0.8419243986  0.9822635135  0.9120631342  0.8405048766  0.8461538462  29.660144181  0.6833167717  8.1240329742  1800          0.7363919298 
0.9943357364  0.8288659794  0.9730813288  0.8373424971  0.9893018018  0.9222096956  0.8433734940  0.8438576349  34.603501544  0.5731548384  8.1240329742  2100          0.7426030445 
Best model upto now
0.9958805355  0.8494845361  0.9808132875  0.8465063001  0.9929617117  0.9177001127  0.8473895582  0.8415614237  39.546858908  0.5956739404  8.1240329742  2400          0.7704186869 
Best model upto now
0.9948506694  0.8556701031  0.9805269187  0.8384879725  0.9940878378  0.9278466742  0.8471026965  0.8404133180  44.490216271  0.5278313899  8.1240329742  2700          0.7820210020 
0.9958805355  0.8474226804  0.9825315006  0.8407789233  0.9943693694  0.9267192785  0.8476764200  0.8358208955  49.433573635  0.5596241965  8.1240329742  3000          0.7788925250 
Best model upto now
0.9979402678  0.8577319588  0.9833906071  0.8430698740  0.9954954955  0.9312288613  0.8462421113  0.8346727899  54.376930999  0.6034654179  8.1240329742  3300          0.7880605292 
0.9958805355  0.8494845361  0.9851088202  0.8499427262  0.9960585586  0.9312288613  0.8459552496  0.8323765786  59.320288362  0.5366774302  8.1240329742  3600          0.8002338966 
0.9958805355  0.8412371134  0.9853951890  0.8510882016  0.9957770270  0.9301014656  0.8448078026  0.8266360505  64.263645726  0.5227561027  8.1240329742  3900          0.8033262412 
Best model upto now
0.9969104016  0.8639175258  0.9865406644  0.8522336770  0.9980292793  0.9267192785  0.8450946644  0.8277841561  69.207003089  0.5011924769  8.1240329742  4200          0.7902961906 
0.9963954686  0.8494845361  0.9879725086  0.8487972509  0.9966216216  0.9289740699  0.8468158348  0.8312284730  74.150360453  0.5243521083  8.1240329742  4500          0.7856861242 
0.9958805355  0.8515463918  0.9859679267  0.8430698740  0.9960585586  0.9233370913  0.8436603557  0.8243398393  79.093717816  0.5362727776  8.1240329742  4800          0.7794933780 
0.9953656025  0.8515463918  0.9853951890  0.8396334479  0.9966216216  0.9301014656  0.8390705680  0.8289322618  82.389289392  0.4829885549  8.1240329742  5000          0.7163649297 
