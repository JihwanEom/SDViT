Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0,"EMA":"True","CutMix":"True"}
	hparams_seed: 0
	output_dir: ./domainbed/PACS/ERM_ViT_Deit-S/EMA_CutMix/c615bfbea681cfcdff77cb6cbfaec3f0
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 244401472
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 1
	uda_holdout_fraction: 0
HParams:
	CutMix: True
	EMA: True
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_env: [3]
	weight_decay: 0.0
device: cuda
/local_ssd2/jeom/SDViT/domainbed/algorithms.py:196: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)
  t_param.data.mul_(self.ema_decay).add_(1 - self.ema_decay, s_param.data)
Best model upto now
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1116534472  0.1222493888  0.1444562900  0.2072649573  0.0800898204  0.0838323353  0.2315521628  0.2292993631  0.0000000000  2.2770829201  7.8689551353  0             6.2700831890 
Best model upto now
0.6076876144  0.5721271394  0.5330490405  0.5128205128  0.7672155689  0.7694610778  0.3447837150  0.3401273885  7.1856287425  0.5189760604  8.1179094315  300           0.3912824559 
Best model upto now
0.9286150092  0.8875305623  0.8667377399  0.8311965812  0.9790419162  0.9760479042  0.5524809160  0.5757961783  14.371257485  0.3453005272  8.1194505692  600           0.4123982636 
Best model upto now
0.9841366687  0.9559902200  0.9717484009  0.9166666667  0.9955089820  0.9910179641  0.6692111959  0.6700636943  21.556886227  0.3241922527  8.1194505692  900           0.4110393254 
Best model upto now
0.9945088469  0.9731051345  0.9941364606  0.9465811966  0.9985029940  0.9970059880  0.7286895674  0.7146496815  28.742514970  0.3394248736  8.1196489334  1200          0.4261754036 
Best model upto now
0.9993898719  0.9682151589  0.9962686567  0.9594017094  1.0000000000  0.9910179641  0.7519083969  0.7350318471  35.928143712  0.2992499882  8.1204576492  1500          0.4545631989 
Best model upto now
1.0000000000  0.9657701711  0.9994669510  0.9743589744  1.0000000000  0.9970059880  0.7566793893  0.7350318471  43.113772455  0.2742139164  8.1204576492  1800          0.5027322213 
Best model upto now
1.0000000000  0.9779951100  0.9989339019  0.9786324786  1.0000000000  0.9910179641  0.7624045802  0.7286624204  50.299401197  0.2842122342  8.1204576492  2100          0.5130502971 
Best model upto now
0.9993898719  0.9755501222  0.9994669510  0.9722222222  1.0000000000  1.0000000000  0.7655852417  0.7312101911  57.485029940  0.2855717731  8.1204576492  2400          0.5712194141 
Best model upto now
1.0000000000  0.9755501222  0.9994669510  0.9786324786  1.0000000000  1.0000000000  0.7735368957  0.7337579618  64.670658682  0.2793164250  8.1204576492  2700          0.6225028793 
Best model upto now
1.0000000000  0.9779951100  1.0000000000  0.9829059829  1.0000000000  1.0000000000  0.7763994911  0.7426751592  71.856287425  0.2835821714  8.1204576492  3000          0.6248878837 
1.0000000000  0.9755501222  1.0000000000  0.9850427350  1.0000000000  0.9940119760  0.7805343511  0.7401273885  79.041916167  0.2749240072  8.1216783524  3300          0.6229805446 
1.0000000000  0.9755501222  1.0000000000  0.9807692308  1.0000000000  1.0000000000  0.7830788804  0.7541401274  86.227544910  0.2962432161  8.1216783524  3600          0.6235277669 
Best model upto now
1.0000000000  0.9804400978  1.0000000000  0.9807692308  1.0000000000  1.0000000000  0.7878498728  0.7554140127  93.413173652  0.2772081132  8.1216783524  3900          0.6205241561 
1.0000000000  0.9706601467  0.9994669510  0.9807692308  1.0000000000  0.9970059880  0.7891221374  0.7579617834  100.59880239  0.2568193571  8.1216783524  4200          0.6243121926 
1.0000000000  0.9779951100  1.0000000000  0.9850427350  1.0000000000  0.9970059880  0.7926208651  0.7566878981  107.78443113  0.2678849490  8.1216783524  4500          0.6230024616 
0.9993898719  0.9657701711  1.0000000000  0.9829059829  1.0000000000  0.9940119760  0.7948473282  0.7579617834  114.97005988  0.2605516625  8.1216783524  4800          0.6227882560 
1.0000000000  0.9657701711  1.0000000000  0.9850427350  1.0000000000  0.9940119760  0.7961195929  0.7617834395  119.76047904  0.2972297203  8.1216783524  5000          0.6242689240 
