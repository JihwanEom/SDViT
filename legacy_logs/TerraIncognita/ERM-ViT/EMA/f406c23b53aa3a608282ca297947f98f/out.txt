Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: TerraIncognita
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0,"EMA":"True"}
	hparams_seed: 0
	output_dir: ./domainbed/TerraIncognita/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/EMA/f406c23b53aa3a608282ca297947f98f
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 248749643
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: True
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_env: [1]
	weight_decay: 0.0
device: cuda
/local_ssd2/jeom/SDViT/domainbed/algorithms.py:199: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)
  t_param.data.mul_(self.ema_decay).add_(1 - self.ema_decay, s_param.data)
Best model upto now
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1120485104  0.1097046414  0.0263191681  0.0261941448  0.0516372796  0.0591939547  0.0503505417  0.0493197279  0.0000000000  2.6261651516  7.8689732552  0             1.0994594097 
Best model upto now
0.5979435803  0.5928270042  0.0514828604  0.0518746790  0.2934508816  0.3148614610  0.3365200765  0.3341836735  3.0226700252  0.7922594400  8.1185050011  300           0.5484955875 
Best model upto now
0.7624571579  0.7489451477  0.0910258056  0.0965588084  0.6171284635  0.6108312343  0.5852984916  0.5663265306  6.0453400504  0.4564924662  8.1185050011  600           0.7546614639 
Best model upto now
0.8660690746  0.8481012658  0.1798690461  0.1926040062  0.7591309824  0.7531486146  0.7272147865  0.6989795918  9.0680100756  0.3510267554  8.1185050011  900           0.7187279900 
Best model upto now
0.9127339837  0.8850210970  0.2696109899  0.2737544941  0.8221032746  0.8022670025  0.7922243467  0.7670068027  12.090680100  0.2942698566  8.1191458702  1200          0.6944353112 
Best model upto now
0.9412074875  0.9208860759  0.3136474515  0.3312788906  0.8683879093  0.8236775819  0.8451242830  0.8035714286  15.113350125  0.2451814946  8.1198782921  1500          0.7005549097 
Best model upto now
0.9591352491  0.9314345992  0.3375272821  0.3461736004  0.9083753149  0.8539042821  0.8767792649  0.8163265306  18.136020151  0.2169638336  8.1198782921  1800          0.6894318183 
Best model upto now
0.9694173477  0.9409282700  0.3510078316  0.3554185927  0.9282115869  0.8702770781  0.8956872743  0.8452380952  21.158690176  0.1933601432  8.1198782921  2100          0.6876230907 
Best model upto now
0.9723174268  0.9504219409  0.3614071126  0.3703133025  0.9382871537  0.8765743073  0.9148077332  0.8520408163  24.181360201  0.1702684569  8.1198782921  2400          0.6524744527 
Best model upto now
0.9781175850  0.9493670886  0.3661574015  0.3749357987  0.9502518892  0.8891687657  0.9237306140  0.8664965986  27.204030226  0.1623330011  8.1199851036  2700          0.7028655497 
Best model upto now
0.9844450303  0.9514767932  0.3719347798  0.3841807910  0.9587531486  0.8904282116  0.9279796048  0.8801020408  30.226700251  0.1514137160  8.1206564903  3000          0.6968135873 
Best model upto now
0.9852359610  0.9578059072  0.3811785852  0.3939393939  0.9619017632  0.9055415617  0.9354153389  0.8860544218  33.249370277  0.1403765403  8.1229147911  3300          0.6999963506 
Best model upto now
0.9894542578  0.9609704641  0.3858004879  0.3949666153  0.9650503778  0.9093198992  0.9422137242  0.8860544218  36.272040302  0.1285282085  8.1229147911  3600          0.7252126551 
0.9897179014  0.9588607595  0.3881114392  0.3995891115  0.9663098237  0.9181360202  0.9477374124  0.8784013605  39.294710327  0.1200687767  8.1229147911  3900          0.7285184924 
0.9870814659  0.9567510549  0.3961997689  0.4067796610  0.9738664987  0.9093198992  0.9494370087  0.8792517007  42.317380352  0.1215944390  8.1229147911  4200          0.7280220691 
Best model upto now
0.9889269707  0.9630801688  0.4082680704  0.4221879815  0.9738664987  0.9156171285  0.9528362014  0.8911564626  45.340050377  0.1102167056  8.1229147911  4500          0.6922049785 
Best model upto now
0.9891906143  0.9672995781  0.4055719605  0.4232152029  0.9773299748  0.9105793451  0.9566602932  0.8954081633  48.362720403  0.1117444777  8.1229147911  4800          0.6587863501 
Best model upto now
0.9873451094  0.9651898734  0.4028758506  0.4227015922  0.9751259446  0.9219143577  0.9547482473  0.8954081633  50.377833753  0.1097979870  8.1229147911  5000          0.6413990462 
