Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: TerraIncognita
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0,"EMA":"True","CutMix":"True"}
	hparams_seed: 0
	output_dir: ./domainbed/TerraIncognita/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/EMA_CutMix/60c2ae4ca729b353f79812c37ff8f10f
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 1005881598
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 1
	uda_holdout_fraction: 0
HParams:
	CutMix: True
	EMA: True
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_env: [0]
	weight_decay: 0.0
device: cuda
/local_ssd2/jeom/SDViT/domainbed/algorithms.py:199: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)
  t_param.data.mul_(self.ema_decay).add_(1 - self.ema_decay, s_param.data)
Best model upto now
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.0416556815  0.0495780591  0.1250481448  0.1196712892  0.0585642317  0.0705289673  0.0393031655  0.0374149660  0.0000000000  2.3541545868  7.8689751625  0             3.3502337933 
Best model upto now
0.0508832059  0.0611814346  0.5376813455  0.5254237288  0.4159319899  0.4483627204  0.4079031230  0.4209183673  3.0226700252  1.1600591289  8.1179423332  300           0.7192873772 
Best model upto now
0.1674136567  0.1824894515  0.7359096161  0.7241910632  0.6523929471  0.6763224181  0.6105799873  0.5833333333  6.0453400504  0.8889459656  8.1194834709  600           0.4623862092 
Best model upto now
0.3253361455  0.3438818565  0.8020285017  0.7889060092  0.7525188917  0.7493702771  0.7301890801  0.6904761905  9.0680100756  0.8150346686  8.1194834709  900           0.3808864379 
Best model upto now
0.4331663591  0.4683544304  0.8452946463  0.8407806882  0.8088790932  0.8085642317  0.7996600807  0.7585034014  12.090680100  0.7995686592  8.1196818352  1200          0.3955796377 
Best model upto now
0.4893224361  0.5369198312  0.8686609321  0.8572162301  0.8523299748  0.8236775819  0.8355640535  0.7840136054  15.113350125  0.7044116971  8.1204905510  1500          0.4038750935 
Best model upto now
0.5201687319  0.5727848101  0.8879188599  0.8767334361  0.8822418136  0.8513853904  0.8610579987  0.8018707483  18.136020151  0.7017712160  8.1204905510  1800          0.3901584872 
Best model upto now
0.5433693646  0.5875527426  0.9047374503  0.8777606574  0.9042821159  0.8639798489  0.8782664117  0.8316326531  21.158690176  0.6754154163  8.1204905510  2100          0.3956347354 
Best model upto now
0.5533878197  0.5928270042  0.9170625241  0.8921417565  0.9190806045  0.8589420655  0.8882515403  0.8341836735  24.181360201  0.6089289517  8.1205515862  2400          0.3854066658 
Best model upto now
0.5568151859  0.5959915612  0.9237386057  0.9049820236  0.9332493703  0.8841309824  0.9039728065  0.8528911565  27.204030226  0.6121678021  8.1205515862  2700          0.3967284767 
Best model upto now
0.5626153441  0.6075949367  0.9325972525  0.9111453518  0.9436397985  0.8866498741  0.9150201827  0.8647959184  30.226700251  0.6242541670  8.1205515862  3000          0.3909996907 
Best model upto now
0.5610334827  0.6170886076  0.9410707408  0.9142270159  0.9480478589  0.8979848866  0.9145952836  0.8622448980  33.249370277  0.6123382672  8.1205515862  3300          0.3959368396 
Best model upto now
0.5760611653  0.6191983122  0.9445371678  0.9173086800  0.9521410579  0.9030226700  0.9245804122  0.8664965986  36.272040302  0.5694004803  8.1205515862  3600          0.3923632979 
Best model upto now
0.5810703928  0.6234177215  0.9499293876  0.9296353364  0.9628463476  0.8967254408  0.9313787975  0.8724489796  39.294710327  0.5719615658  8.1205515862  3900          0.4009559329 
Best model upto now
0.5850250461  0.6276371308  0.9560919245  0.9255264510  0.9666246851  0.9017632242  0.9347779902  0.8809523810  42.317380352  0.5939086034  8.1205515862  4200          0.3994313256 
Best model upto now
0.5902979172  0.6265822785  0.9594299653  0.9311761685  0.9641057935  0.9005037783  0.9388145315  0.8911564626  45.340050377  0.5378770884  8.1205515862  4500          0.3972398106 
Best model upto now
0.5831795413  0.6276371308  0.9618693029  0.9352850539  0.9675692695  0.9231738035  0.9415763756  0.8894557823  48.362720403  0.5558218324  8.1205515862  4800          0.3959524663 
0.5800158186  0.6244725738  0.9632815509  0.9378531073  0.9644206549  0.9219143577  0.9398767793  0.8852040816  50.377833753  0.4599257666  8.1205515862  5000          0.3971803868 
