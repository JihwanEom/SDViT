Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: TerraIncognita
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0,"EMA":"True","CutMix":"True"}
	hparams_seed: 0
	output_dir: ./domainbed/TerraIncognita/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/EMA_CutMix/7153e9281bfe1b99c06a9d9ec8cfc976
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 1911425495
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 2
	uda_holdout_fraction: 0
HParams:
	CutMix: True
	EMA: True
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_env: [3]
	weight_decay: 0.0
device: cuda
/local_ssd2/jeom/SDViT/domainbed/algorithms.py:199: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)
  t_param.data.mul_(self.ema_decay).add_(1 - self.ema_decay, s_param.data)
Best model upto now
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.0585288690  0.0537974684  0.0175889074  0.0251669235  0.0412468514  0.0302267003  0.0696834502  0.0603741497  0.0000000000  2.5931248665  7.8689751625  0             3.9517586231 
Best model upto now
0.5842341155  0.5991561181  0.5905764540  0.5803800719  0.3186397985  0.3274559194  0.1495644784  0.1479591837  3.0226700252  1.0205323461  8.1179423332  300           0.6714820870 
Best model upto now
0.7571842868  0.7647679325  0.7443831044  0.7308680021  0.5875314861  0.5579345088  0.2836201402  0.2933673469  6.0453400504  0.7943402515  8.1194834709  600           0.6710525147 
Best model upto now
0.8549960453  0.8333333333  0.8022852741  0.7919876733  0.7506297229  0.7166246851  0.3866581687  0.3869047619  9.0680100756  0.7188535197  8.1194834709  900           0.7472608860 
Best model upto now
0.9100975481  0.8723628692  0.8431120811  0.8299948639  0.8277707809  0.7682619647  0.4289356278  0.4200680272  12.090680100  0.6693881668  8.1205973625  1200          0.7810832151 
Best model upto now
0.9390983390  0.9061181435  0.8707151111  0.8489984592  0.8617758186  0.8198992443  0.4510303803  0.4438775510  15.113350125  0.6323187649  8.1205973625  1500          0.7734751527 
Best model upto now
0.9517532296  0.9187763713  0.8870201566  0.8685156651  0.8872795970  0.8249370277  0.4591034629  0.4430272109  18.136020151  0.5686384060  8.1205973625  1800          0.7686015749 
Best model upto now
0.9604534669  0.9293248945  0.8989600719  0.8803287108  0.9014483627  0.8602015113  0.4618653070  0.4447278912  21.158690176  0.5920822503  8.1205973625  2100          0.7752039115 
Best model upto now
0.9715264962  0.9440928270  0.9142380280  0.8900873138  0.9206549118  0.8652392947  0.4659018483  0.4438775510  24.181360201  0.5121799656  8.1220164299  2400          0.7719028068 
Best model upto now
0.9744265753  0.9514767932  0.9227115162  0.8962506420  0.9323047859  0.8715365239  0.4639898024  0.4362244898  27.204030226  0.5472066915  8.1220164299  2700          0.7754012791 
Best model upto now
0.9789085157  0.9440928270  0.9322120940  0.9070364664  0.9445843829  0.8803526448  0.4557042702  0.4277210884  30.226700251  0.5277495533  8.1220164299  3000          0.7767790325 
Best model upto now
0.9789085157  0.9514767932  0.9381178585  0.9096045198  0.9464735516  0.8879093199  0.4506054812  0.4234693878  33.249370277  0.5251629843  8.1220164299  3300          0.8102480038 
Best model upto now
0.9807540206  0.9588607595  0.9424829888  0.9229583975  0.9559193955  0.9030226700  0.4472062885  0.4226190476  36.272040302  0.4786068864  8.1220164299  3600          0.9566214005 
0.9823358819  0.9514767932  0.9508280909  0.9234720082  0.9565491184  0.8992443325  0.4463564903  0.4200680272  39.294710327  0.4508855247  8.1220164299  3900          1.0393132552 
0.9836540997  0.9578059072  0.9526254975  0.9244992296  0.9666246851  0.9017632242  0.4461440408  0.4268707483  42.317380352  0.5055480828  8.1220164299  4200          1.0458660698 
0.9862905352  0.9578059072  0.9569906278  0.9378531073  0.9612720403  0.8891687657  0.4412577013  0.4149659864  45.340050377  0.4666354345  8.1220164299  4500          1.0489615806 
Best model upto now
0.9889269707  0.9599156118  0.9641802542  0.9260400616  0.9672544081  0.9206549118  0.4378585086  0.4107142857  48.362720403  0.4445106237  8.1220164299  4800          1.0483269564 
Best model upto now
0.9886633272  0.9578059072  0.9640518680  0.9332306112  0.9650503778  0.9269521411  0.4346717655  0.4158163265  50.377833753  0.4632484710  8.1220164299  5000          1.0484631217 
