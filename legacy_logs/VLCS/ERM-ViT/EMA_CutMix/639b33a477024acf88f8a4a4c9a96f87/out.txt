Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0,"EMA":"True","CutMix":"True"}
	hparams_seed: 0
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/EMA_CutMix/639b33a477024acf88f8a4a4c9a96f87
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: True
	EMA: True
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_env: [1]
	weight_decay: 0.0
device: cuda
/local_ssd2/jeom/SDViT/domainbed/algorithms.py:196: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)
  t_param.data.mul_(self.ema_decay).add_(1 - self.ema_decay, s_param.data)
Best model upto now
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1148409894  0.1060070671  0.1647058824  0.1638418079  0.1561309977  0.1448170732  0.2247315809  0.2607407407  0.0000000000  1.7143535614  7.8689379692  0             1.8482542038 
Best model upto now
0.9028268551  0.8798586572  0.5515294118  0.5555555556  0.6431835491  0.5960365854  0.6856719733  0.6740740741  8.4805653710  0.4839364494  8.1178836823  300           0.3852123880 
Best model upto now
0.9893992933  0.9893992933  0.6160000000  0.6101694915  0.8354912414  0.7652439024  0.8744909293  0.8592592593  16.961130742  0.3390567150  8.1194248199  600           0.3892195916 
Best model upto now
1.0000000000  1.0000000000  0.6390588235  0.6403013183  0.9009900990  0.8170731707  0.9289152166  0.8725925926  25.441696113  0.3247053912  8.1205844879  900           0.3962650243 
Best model upto now
1.0000000000  1.0000000000  0.6442352941  0.6440677966  0.9485910129  0.8368902439  0.9607552758  0.8814814815  33.922261484  0.2778093432  8.1205844879  1200          0.3963285017 
1.0000000000  1.0000000000  0.6494117647  0.6440677966  0.9771515613  0.8246951220  0.9803776379  0.8874074074  42.402826855  0.2485525882  8.1205844879  1500          0.3944561402 
1.0000000000  1.0000000000  0.6480000000  0.6421845574  0.9870525514  0.8292682927  0.9918548686  0.8859259259  50.883392226  0.2763892392  8.1205844879  1800          0.3947977026 
Best model upto now
1.0000000000  1.0000000000  0.6470588235  0.6478342750  0.9931454684  0.8429878049  0.9962976675  0.8859259259  59.363957597  0.2694415524  8.1205844879  2100          0.3990010182 
Best model upto now
1.0000000000  1.0000000000  0.6414117647  0.6403013183  0.9977151561  0.8445121951  0.9966679008  0.8874074074  67.844522968  0.2706987156  8.1205844879  2400          0.3940000486 
Best model upto now
1.0000000000  1.0000000000  0.6357647059  0.6346516008  0.9988575781  0.8429878049  0.9977786005  0.8933333333  76.325088339  0.2310430736  8.1205844879  2700          0.3938453452 
1.0000000000  1.0000000000  0.6352941176  0.6384180791  0.9996191927  0.8368902439  0.9985190670  0.8859259259  84.805653710  0.2596236317  8.1205844879  3000          0.3972308834 
1.0000000000  1.0000000000  0.6315294118  0.6327683616  0.9996191927  0.8399390244  0.9992595335  0.8829629630  93.286219081  0.2407571458  8.1205844879  3300          0.3950778294 
1.0000000000  1.0000000000  0.6225882353  0.6252354049  1.0000000000  0.8445121951  0.9996297668  0.8859259259  101.76678445  0.2447028750  8.1205844879  3600          0.3944190661 
1.0000000000  1.0000000000  0.6207058824  0.6195856874  1.0000000000  0.8323170732  0.9996297668  0.8948148148  110.24734982  0.2450012493  8.1205844879  3900          0.3930605992 
Best model upto now
1.0000000000  1.0000000000  0.6202352941  0.6195856874  1.0000000000  0.8490853659  1.0000000000  0.8918518519  118.72791519  0.2469351806  8.1205844879  4200          0.3912910374 
1.0000000000  1.0000000000  0.6197647059  0.6195856874  1.0000000000  0.8246951220  1.0000000000  0.8711111111  127.20848056  0.2529147729  8.1205844879  4500          0.4446780078 
1.0000000000  1.0000000000  0.6211764706  0.6195856874  1.0000000000  0.8201219512  1.0000000000  0.8859259259  135.68904593  0.2409963248  8.1205844879  4800          0.5198929238 
1.0000000000  1.0000000000  0.6188235294  0.6214689266  1.0000000000  0.8323170732  0.9996297668  0.8814814815  141.34275618  0.2183540703  8.1205844879  5000          0.5371960866 
