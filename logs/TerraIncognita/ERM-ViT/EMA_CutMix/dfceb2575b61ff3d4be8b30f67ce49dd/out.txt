Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: TerraIncognita
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0,"EMA":"True","CutMix":"True"}
	hparams_seed: 0
	output_dir: ./domainbed/TerraIncognita/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/EMA_CutMix/dfceb2575b61ff3d4be8b30f67ce49dd
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 349429347
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 2
	uda_holdout_fraction: 0
HParams:
	CutMix: True
	EMA: True
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_env: [0]
	weight_decay: 0.0
device: cuda
/local_ssd2/jeom/SDViT/domainbed/algorithms.py:199: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)
  t_param.data.mul_(self.ema_decay).add_(1 - self.ema_decay, s_param.data)
Best model upto now
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1919325073  0.1803797468  0.0694569264  0.0826913200  0.0346347607  0.0138539043  0.0444019545  0.0399659864  0.0000000000  2.6132602692  7.8689732552  0             2.7184317112 
Best model upto now
0.3126812549  0.2837552743  0.5971241494  0.5839753467  0.3517002519  0.3236775819  0.3906947100  0.3860544218  3.0226700252  1.1608905991  8.1185069084  300           0.7400929387 
Best model upto now
0.3200632745  0.3111814346  0.7477211452  0.7318952234  0.6124055416  0.5730478589  0.6229020608  0.6130952381  6.0453400504  0.9190719358  8.1185069084  600           0.7381657322 
Best model upto now
0.3909833905  0.3934599156  0.8096032867  0.8012326656  0.7399244332  0.7052896725  0.7151051625  0.6981292517  9.0680100756  0.8296839820  8.1185069084  900           0.8090473175 
Best model upto now
0.4339572897  0.4367088608  0.8445243292  0.8299948639  0.8117128463  0.7821158690  0.7813894200  0.7576530612  12.090680100  0.7335667123  8.1191477776  1200          0.7984750438 
Best model upto now
0.4771948326  0.4915611814  0.8699447939  0.8525937339  0.8570528967  0.7959697733  0.8230295305  0.7925170068  15.113350125  0.7429753242  8.1198801994  1500          0.8057745028 
Best model upto now
0.5093593462  0.5337552743  0.8868917704  0.8654340010  0.8882241814  0.8211586902  0.8534098152  0.8095238095  18.136020151  0.7253873642  8.1198801994  1800          0.8159369246 
Best model upto now
0.5394147113  0.5580168776  0.9024264989  0.8767334361  0.8989294710  0.8375314861  0.8676439346  0.8222789116  21.158690176  0.6548770842  8.1198801994  2100          0.8067733892 
Best model upto now
0.5449512259  0.5643459916  0.9179612274  0.8854648177  0.9190806045  0.8513853904  0.8867643935  0.8333333333  24.181360201  0.6416802333  8.1198801994  2400          0.8139831146 
Best model upto now
0.5523332455  0.5696202532  0.9248940814  0.8988186954  0.9319899244  0.8702770781  0.9001487147  0.8596938776  27.204030226  0.6251943602  8.1198801994  2700          0.8148310216 
Best model upto now
0.5502240970  0.5759493671  0.9328540249  0.9013867488  0.9401763224  0.8715365239  0.9122583386  0.8605442177  30.226700251  0.6084284160  8.1198801994  3000          0.9535578219 
Best model upto now
0.5486422357  0.5706751055  0.9411991270  0.9085772984  0.9471032746  0.8853904282  0.9173571277  0.8732993197  33.249370277  0.5981168906  8.1242747307  3300          1.2531344573 
Best model upto now
0.5478513050  0.5696202532  0.9437668507  0.9147406266  0.9508816121  0.8942065491  0.9279796048  0.8690476190  36.272040302  0.5611054218  8.1242747307  3600          1.3471593364 
Best model upto now
0.5462694437  0.5611814346  0.9509564771  0.9209039548  0.9596977330  0.8967254408  0.9315912471  0.8886054422  39.294710327  0.5816190735  8.1242747307  3900          1.3474830055 
Best model upto now
0.5454785131  0.5632911392  0.9545512903  0.9280945044  0.9578085642  0.8992443325  0.9396643297  0.8835034014  42.317380352  0.5510445150  8.1242747307  4200          1.3469807156 
Best model upto now
0.5362509887  0.5601265823  0.9598151239  0.9311761685  0.9644206549  0.9105793451  0.9394518802  0.8877551020  45.340050377  0.5440965505  8.1242747307  4500          1.3506041861 
Best model upto now
0.5338781967  0.5516877637  0.9645654128  0.9337442219  0.9672544081  0.9231738035  0.9430635224  0.8894557823  48.362720403  0.5525845479  8.1242747307  4800          1.4305822420 
Best model upto now
0.5209596625  0.5495780591  0.9645654128  0.9378531073  0.9700881612  0.9282115869  0.9492245592  0.8962585034  50.377833753  0.5714824592  8.1242747307  5000          1.6721045899 
