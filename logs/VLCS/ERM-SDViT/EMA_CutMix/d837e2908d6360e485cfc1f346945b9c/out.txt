Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	algorithm: ERM_SDViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0,"EMA":"True","CutMix":"True"}
	hparams_seed: 0
	output_dir: ./domainbed/VLCS/ERM_SDViT_ours/ERM_ViT_ours_DeiT-small/EMA_CutMix/d837e2908d6360e485cfc1f346945b9c
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 68040364
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 1
	uda_holdout_fraction: 0
HParams:
	CutMix: True
	EMA: True
	EMA_decay: 0.999
	KL_Div_Temperature: 3.0
	RB_loss_weight: 0.5
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_env: [3]
	weight_decay: 0.0
device: cuda
/local_ssd2/jeom/SDViT/domainbed/algorithms.py:269: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)
  t_param.data.mul_(self.ema_decay).add_(1 - self.ema_decay, s_param.data)
Best model upto now
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.6625441696  0.6537102473  0.1364705882  0.1581920904  0.1473724296  0.1509146341  0.2125138837  0.2177777778  0.0000000000  1.6555074453  7.8686480522  0             1.8449535370 
Best model upto now
0.9567137809  0.9540636042  0.5976470588  0.5743879473  0.6264280274  0.5945121951  0.5094409478  0.4962962963  8.4805653710  0.6191473316  8.1178989410  300           1.0406805460 
Best model upto now
0.9955830389  1.0000000000  0.7425882353  0.6967984934  0.8271134806  0.7957317073  0.7145501666  0.6844444444  16.961130742  0.5008011574  8.1179141998  600           0.9723947668 
Best model upto now
1.0000000000  0.9964664311  0.8155294118  0.7325800377  0.8907083016  0.8399390244  0.7711958534  0.7451851852  25.441696113  0.4363972482  8.1180210114  900           0.9890757902 
1.0000000000  0.9964664311  0.8757647059  0.7495291902  0.9371667936  0.8185975610  0.7937800815  0.7807407407  33.922261484  0.3739115712  8.1198978424  1200          0.9612726458 
Best model upto now
1.0000000000  1.0000000000  0.9265882353  0.7514124294  0.9668697639  0.8506097561  0.8059977786  0.7896296296  42.402826855  0.3899293817  8.1198978424  1500          0.9932136599 
1.0000000000  1.0000000000  0.9604705882  0.7589453861  0.9862909368  0.8429878049  0.8078489448  0.7896296296  50.883392226  0.3818572530  8.1198978424  1800          0.9947278388 
1.0000000000  1.0000000000  0.9769411765  0.7627118644  0.9927646611  0.8277439024  0.8034061459  0.8044444444  59.363957597  0.3534780340  8.1198978424  2100          0.9799492701 
Best model upto now
1.0000000000  1.0000000000  0.9920000000  0.7570621469  0.9961919269  0.8506097561  0.7989633469  0.8059259259  67.844522968  0.3378014075  8.1198978424  2400          1.0141019726 
Best model upto now
1.0000000000  1.0000000000  0.9971764706  0.7740112994  0.9958111196  0.8445121951  0.7952610144  0.7985185185  76.325088339  0.3419895423  8.1198978424  2700          0.9306372754 
1.0000000000  0.9964664311  0.9995294118  0.7796610169  0.9988575781  0.8368902439  0.7956312477  0.7970370370  84.805653710  0.3649087393  8.1207218170  3000          0.9591023262 
1.0000000000  0.9964664311  0.9990588235  0.7514124294  0.9992383854  0.8414634146  0.7934098482  0.7896296296  93.286219081  0.3360502259  8.1207218170  3300          0.9521798245 
Best model upto now
1.0000000000  1.0000000000  0.9995294118  0.7815442561  0.9996191927  0.8429878049  0.7963717142  0.7881481481  101.76678445  0.3523062003  8.1207218170  3600          0.8769984301 
1.0000000000  0.9964664311  1.0000000000  0.7909604520  0.9996191927  0.8307926829  0.7934098482  0.7807407407  110.24734982  0.2907746107  8.1207218170  3900          0.8224274906 
1.0000000000  0.9964664311  1.0000000000  0.7777777778  0.9996191927  0.8445121951  0.7900777490  0.7748148148  118.72791519  0.3180279088  8.1207218170  4200          0.5585958171 
Best model upto now
1.0000000000  0.9964664311  0.9995294118  0.7777777778  1.0000000000  0.8567073171  0.7878563495  0.7703703704  127.20848056  0.3209740327  8.1207218170  4500          0.5717666237 
1.0000000000  0.9964664311  1.0000000000  0.7721280603  1.0000000000  0.8353658537  0.7904479822  0.7718518519  135.68904593  0.3375150343  8.1207218170  4800          0.5677505128 
1.0000000000  0.9964664311  1.0000000000  0.7645951036  1.0000000000  0.8460365854  0.7863754165  0.7748148148  141.34275618  0.2994745874  8.1207218170  5000          0.5330052662 
