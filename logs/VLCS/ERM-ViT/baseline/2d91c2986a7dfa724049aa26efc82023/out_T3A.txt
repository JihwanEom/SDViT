Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 276, in <module>
    algorithm.network = DataParallelPassthrough(algorithm.student)
NameError: name 'DataParallelPassthrough' is not defined
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 276, in <module>
    algorithm.network = DataParallelPassthrough(algorithm.student)
NameError: name 'DataParallelPassthrough' is not defined
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 289, in <module>
    algorithm.load_state_dict(algorithm_dict)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ERM_ViT:
	Missing key(s) in state_dict: "network.module.cls_token", "network.module.pos_embed", "network.module.patch_embed.proj.weight", "network.module.patch_embed.proj.bias", "network.module.blocks.0.norm1.weight", "network.module.blocks.0.norm1.bias", "network.module.blocks.0.attn.qkv.weight", "network.module.blocks.0.attn.qkv.bias", "network.module.blocks.0.attn.proj.weight", "network.module.blocks.0.attn.proj.bias", "network.module.blocks.0.norm2.weight", "network.module.blocks.0.norm2.bias", "network.module.blocks.0.mlp.fc1.weight", "network.module.blocks.0.mlp.fc1.bias", "network.module.blocks.0.mlp.fc2.weight", "network.module.blocks.0.mlp.fc2.bias", "network.module.blocks.1.norm1.weight", "network.module.blocks.1.norm1.bias", "network.module.blocks.1.attn.qkv.weight", "network.module.blocks.1.attn.qkv.bias", "network.module.blocks.1.attn.proj.weight", "network.module.blocks.1.attn.proj.bias", "network.module.blocks.1.norm2.weight", "network.module.blocks.1.norm2.bias", "network.module.blocks.1.mlp.fc1.weight", "network.module.blocks.1.mlp.fc1.bias", "network.module.blocks.1.mlp.fc2.weight", "network.module.blocks.1.mlp.fc2.bias", "network.module.blocks.2.norm1.weight", "network.module.blocks.2.norm1.bias", "network.module.blocks.2.attn.qkv.weight", "network.module.blocks.2.attn.qkv.bias", "network.module.blocks.2.attn.proj.weight", "network.module.blocks.2.attn.proj.bias", "network.module.blocks.2.norm2.weight", "network.module.blocks.2.norm2.bias", "network.module.blocks.2.mlp.fc1.weight", "network.module.blocks.2.mlp.fc1.bias", "network.module.blocks.2.mlp.fc2.weight", "network.module.blocks.2.mlp.fc2.bias", "network.module.blocks.3.norm1.weight", "network.module.blocks.3.norm1.bias", "network.module.blocks.3.attn.qkv.weight", "network.module.blocks.3.attn.qkv.bias", "network.module.blocks.3.attn.proj.weight", "network.module.blocks.3.attn.proj.bias", "network.module.blocks.3.norm2.weight", "network.module.blocks.3.norm2.bias", "network.module.blocks.3.mlp.fc1.weight", "network.module.blocks.3.mlp.fc1.bias", "network.module.blocks.3.mlp.fc2.weight", "network.module.blocks.3.mlp.fc2.bias", "network.module.blocks.4.norm1.weight", "network.module.blocks.4.norm1.bias", "network.module.blocks.4.attn.qkv.weight", "network.module.blocks.4.attn.qkv.bias", "network.module.blocks.4.attn.proj.weight", "network.module.blocks.4.attn.proj.bias", "network.module.blocks.4.norm2.weight", "network.module.blocks.4.norm2.bias", "network.module.blocks.4.mlp.fc1.weight", "network.module.blocks.4.mlp.fc1.bias", "network.module.blocks.4.mlp.fc2.weight", "network.module.blocks.4.mlp.fc2.bias", "network.module.blocks.5.norm1.weight", "network.module.blocks.5.norm1.bias", "network.module.blocks.5.attn.qkv.weight", "network.module.blocks.5.attn.qkv.bias", "network.module.blocks.5.attn.proj.weight", "network.module.blocks.5.attn.proj.bias", "network.module.blocks.5.norm2.weight", "network.module.blocks.5.norm2.bias", "network.module.blocks.5.mlp.fc1.weight", "network.module.blocks.5.mlp.fc1.bias", "network.module.blocks.5.mlp.fc2.weight", "network.module.blocks.5.mlp.fc2.bias", "network.module.blocks.6.norm1.weight", "network.module.blocks.6.norm1.bias", "network.module.blocks.6.attn.qkv.weight", "network.module.blocks.6.attn.qkv.bias", "network.module.blocks.6.attn.proj.weight", "network.module.blocks.6.attn.proj.bias", "network.module.blocks.6.norm2.weight", "network.module.blocks.6.norm2.bias", "network.module.blocks.6.mlp.fc1.weight", "network.module.blocks.6.mlp.fc1.bias", "network.module.blocks.6.mlp.fc2.weight", "network.module.blocks.6.mlp.fc2.bias", "network.module.blocks.7.norm1.weight", "network.module.blocks.7.norm1.bias", "network.module.blocks.7.attn.qkv.weight", "network.module.blocks.7.attn.qkv.bias", "network.module.blocks.7.attn.proj.weight", "network.module.blocks.7.attn.proj.bias", "network.module.blocks.7.norm2.weight", "network.module.blocks.7.norm2.bias", "network.module.blocks.7.mlp.fc1.weight", "network.module.blocks.7.mlp.fc1.bias", "network.module.blocks.7.mlp.fc2.weight", "network.module.blocks.7.mlp.fc2.bias", "network.module.blocks.8.norm1.weight", "network.module.blocks.8.norm1.bias", "network.module.blocks.8.attn.qkv.weight", "network.module.blocks.8.attn.qkv.bias", "network.module.blocks.8.attn.proj.weight", "network.module.blocks.8.attn.proj.bias", "network.module.blocks.8.norm2.weight", "network.module.blocks.8.norm2.bias", "network.module.blocks.8.mlp.fc1.weight", "network.module.blocks.8.mlp.fc1.bias", "network.module.blocks.8.mlp.fc2.weight", "network.module.blocks.8.mlp.fc2.bias", "network.module.blocks.9.norm1.weight", "network.module.blocks.9.norm1.bias", "network.module.blocks.9.attn.qkv.weight", "network.module.blocks.9.attn.qkv.bias", "network.module.blocks.9.attn.proj.weight", "network.module.blocks.9.attn.proj.bias", "network.module.blocks.9.norm2.weight", "network.module.blocks.9.norm2.bias", "network.module.blocks.9.mlp.fc1.weight", "network.module.blocks.9.mlp.fc1.bias", "network.module.blocks.9.mlp.fc2.weight", "network.module.blocks.9.mlp.fc2.bias", "network.module.blocks.10.norm1.weight", "network.module.blocks.10.norm1.bias", "network.module.blocks.10.attn.qkv.weight", "network.module.blocks.10.attn.qkv.bias", "network.module.blocks.10.attn.proj.weight", "network.module.blocks.10.attn.proj.bias", "network.module.blocks.10.norm2.weight", "network.module.blocks.10.norm2.bias", "network.module.blocks.10.mlp.fc1.weight", "network.module.blocks.10.mlp.fc1.bias", "network.module.blocks.10.mlp.fc2.weight", "network.module.blocks.10.mlp.fc2.bias", "network.module.blocks.11.norm1.weight", "network.module.blocks.11.norm1.bias", "network.module.blocks.11.attn.qkv.weight", "network.module.blocks.11.attn.qkv.bias", "network.module.blocks.11.attn.proj.weight", "network.module.blocks.11.attn.proj.bias", "network.module.blocks.11.norm2.weight", "network.module.blocks.11.norm2.bias", "network.module.blocks.11.mlp.fc1.weight", "network.module.blocks.11.mlp.fc1.bias", "network.module.blocks.11.mlp.fc2.weight", "network.module.blocks.11.mlp.fc2.bias", "network.module.norm.weight", "network.module.norm.bias", "network.module.head.weight", "network.module.head.bias". 
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
> /local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py(289)<module>()
-> if algorithm_dict is not None:
(Pdb) odict_keys(['student.cls_token', 'student.pos_embed', 'student.patch_embed.proj.weight', 'student.patch_embed.proj.bias', 'student.blocks.0.norm1.weight', 'student.blocks.0.norm1.bias', 'student.blocks.0.attn.qkv.weight', 'student.blocks.0.attn.qkv.bias', 'student.blocks.0.attn.proj.weight', 'student.blocks.0.attn.proj.bias', 'student.blocks.0.norm2.weight', 'student.blocks.0.norm2.bias', 'student.blocks.0.mlp.fc1.weight', 'student.blocks.0.mlp.fc1.bias', 'student.blocks.0.mlp.fc2.weight', 'student.blocks.0.mlp.fc2.bias', 'student.blocks.1.norm1.weight', 'student.blocks.1.norm1.bias', 'student.blocks.1.attn.qkv.weight', 'student.blocks.1.attn.qkv.bias', 'student.blocks.1.attn.proj.weight', 'student.blocks.1.attn.proj.bias', 'student.blocks.1.norm2.weight', 'student.blocks.1.norm2.bias', 'student.blocks.1.mlp.fc1.weight', 'student.blocks.1.mlp.fc1.bias', 'student.blocks.1.mlp.fc2.weight', 'student.blocks.1.mlp.fc2.bias', 'student.blocks.2.norm1.weight', 'student.blocks.2.norm1.bias', 'student.blocks.2.attn.qkv.weight', 'student.blocks.2.attn.qkv.bias', 'student.blocks.2.attn.proj.weight', 'student.blocks.2.attn.proj.bias', 'student.blocks.2.norm2.weight', 'student.blocks.2.norm2.bias', 'student.blocks.2.mlp.fc1.weight', 'student.blocks.2.mlp.fc1.bias', 'student.blocks.2.mlp.fc2.weight', 'student.blocks.2.mlp.fc2.bias', 'student.blocks.3.norm1.weight', 'student.blocks.3.norm1.bias', 'student.blocks.3.attn.qkv.weight', 'student.blocks.3.attn.qkv.bias', 'student.blocks.3.attn.proj.weight', 'student.blocks.3.attn.proj.bias', 'student.blocks.3.norm2.weight', 'student.blocks.3.norm2.bias', 'student.blocks.3.mlp.fc1.weight', 'student.blocks.3.mlp.fc1.bias', 'student.blocks.3.mlp.fc2.weight', 'student.blocks.3.mlp.fc2.bias', 'student.blocks.4.norm1.weight', 'student.blocks.4.norm1.bias', 'student.blocks.4.attn.qkv.weight', 'student.blocks.4.attn.qkv.bias', 'student.blocks.4.attn.proj.weight', 'student.blocks.4.attn.proj.bias', 'student.blocks.4.norm2.weight', 'student.blocks.4.norm2.bias', 'student.blocks.4.mlp.fc1.weight', 'student.blocks.4.mlp.fc1.bias', 'student.blocks.4.mlp.fc2.weight', 'student.blocks.4.mlp.fc2.bias', 'student.blocks.5.norm1.weight', 'student.blocks.5.norm1.bias', 'student.blocks.5.attn.qkv.weight', 'student.blocks.5.attn.qkv.bias', 'student.blocks.5.attn.proj.weight', 'student.blocks.5.attn.proj.bias', 'student.blocks.5.norm2.weight', 'student.blocks.5.norm2.bias', 'student.blocks.5.mlp.fc1.weight', 'student.blocks.5.mlp.fc1.bias', 'student.blocks.5.mlp.fc2.weight', 'student.blocks.5.mlp.fc2.bias', 'student.blocks.6.norm1.weight', 'student.blocks.6.norm1.bias', 'student.blocks.6.attn.qkv.weight', 'student.blocks.6.attn.qkv.bias', 'student.blocks.6.attn.proj.weight', 'student.blocks.6.attn.proj.bias', 'student.blocks.6.norm2.weight', 'student.blocks.6.norm2.bias', 'student.blocks.6.mlp.fc1.weight', 'student.blocks.6.mlp.fc1.bias', 'student.blocks.6.mlp.fc2.weight', 'student.blocks.6.mlp.fc2.bias', 'student.blocks.7.norm1.weight', 'student.blocks.7.norm1.bias', 'student.blocks.7.attn.qkv.weight', 'student.blocks.7.attn.qkv.bias', 'student.blocks.7.attn.proj.weight', 'student.blocks.7.attn.proj.bias', 'student.blocks.7.norm2.weight', 'student.blocks.7.norm2.bias', 'student.blocks.7.mlp.fc1.weight', 'student.blocks.7.mlp.fc1.bias', 'student.blocks.7.mlp.fc2.weight', 'student.blocks.7.mlp.fc2.bias', 'student.blocks.8.norm1.weight', 'student.blocks.8.norm1.bias', 'student.blocks.8.attn.qkv.weight', 'student.blocks.8.attn.qkv.bias', 'student.blocks.8.attn.proj.weight', 'student.blocks.8.attn.proj.bias', 'student.blocks.8.norm2.weight', 'student.blocks.8.norm2.bias', 'student.blocks.8.mlp.fc1.weight', 'student.blocks.8.mlp.fc1.bias', 'student.blocks.8.mlp.fc2.weight', 'student.blocks.8.mlp.fc2.bias', 'student.blocks.9.norm1.weight', 'student.blocks.9.norm1.bias', 'student.blocks.9.attn.qkv.weight', 'student.blocks.9.attn.qkv.bias', 'student.blocks.9.attn.proj.weight', 'student.blocks.9.attn.proj.bias', 'student.blocks.9.norm2.weight', 'student.blocks.9.norm2.bias', 'student.blocks.9.mlp.fc1.weight', 'student.blocks.9.mlp.fc1.bias', 'student.blocks.9.mlp.fc2.weight', 'student.blocks.9.mlp.fc2.bias', 'student.blocks.10.norm1.weight', 'student.blocks.10.norm1.bias', 'student.blocks.10.attn.qkv.weight', 'student.blocks.10.attn.qkv.bias', 'student.blocks.10.attn.proj.weight', 'student.blocks.10.attn.proj.bias', 'student.blocks.10.norm2.weight', 'student.blocks.10.norm2.bias', 'student.blocks.10.mlp.fc1.weight', 'student.blocks.10.mlp.fc1.bias', 'student.blocks.10.mlp.fc2.weight', 'student.blocks.10.mlp.fc2.bias', 'student.blocks.11.norm1.weight', 'student.blocks.11.norm1.bias', 'student.blocks.11.attn.qkv.weight', 'student.blocks.11.attn.qkv.bias', 'student.blocks.11.attn.proj.weight', 'student.blocks.11.attn.proj.bias', 'student.blocks.11.norm2.weight', 'student.blocks.11.norm2.bias', 'student.blocks.11.mlp.fc1.weight', 'student.blocks.11.mlp.fc1.bias', 'student.blocks.11.mlp.fc2.weight', 'student.blocks.11.mlp.fc2.bias', 'student.norm.weight', 'student.norm.bias', 'student.head.weight', 'student.head.bias'])
(Pdb) ERM_ViT(
  (student): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
    (head): Linear(in_features=384, out_features=5, bias=True)
  )
  (network): DataParallelPassthrough(
    (module): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (head): Linear(in_features=384, out_features=5, bias=True)
    )
  )
)
(Pdb) Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 289, in <module>
    algorithm.load_state_dict(algorithm_dict)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 289, in <module>
    algorithm.load_state_dict(algorithm_dict)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 289, in <module>
    algorithm.load_state_dict(algorithm_dict)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ERM_ViT:
	Missing key(s) in state_dict: "student.module.cls_token", "student.module.pos_embed", "student.module.patch_embed.proj.weight", "student.module.patch_embed.proj.bias", "student.module.blocks.0.norm1.weight", "student.module.blocks.0.norm1.bias", "student.module.blocks.0.attn.qkv.weight", "student.module.blocks.0.attn.qkv.bias", "student.module.blocks.0.attn.proj.weight", "student.module.blocks.0.attn.proj.bias", "student.module.blocks.0.norm2.weight", "student.module.blocks.0.norm2.bias", "student.module.blocks.0.mlp.fc1.weight", "student.module.blocks.0.mlp.fc1.bias", "student.module.blocks.0.mlp.fc2.weight", "student.module.blocks.0.mlp.fc2.bias", "student.module.blocks.1.norm1.weight", "student.module.blocks.1.norm1.bias", "student.module.blocks.1.attn.qkv.weight", "student.module.blocks.1.attn.qkv.bias", "student.module.blocks.1.attn.proj.weight", "student.module.blocks.1.attn.proj.bias", "student.module.blocks.1.norm2.weight", "student.module.blocks.1.norm2.bias", "student.module.blocks.1.mlp.fc1.weight", "student.module.blocks.1.mlp.fc1.bias", "student.module.blocks.1.mlp.fc2.weight", "student.module.blocks.1.mlp.fc2.bias", "student.module.blocks.2.norm1.weight", "student.module.blocks.2.norm1.bias", "student.module.blocks.2.attn.qkv.weight", "student.module.blocks.2.attn.qkv.bias", "student.module.blocks.2.attn.proj.weight", "student.module.blocks.2.attn.proj.bias", "student.module.blocks.2.norm2.weight", "student.module.blocks.2.norm2.bias", "student.module.blocks.2.mlp.fc1.weight", "student.module.blocks.2.mlp.fc1.bias", "student.module.blocks.2.mlp.fc2.weight", "student.module.blocks.2.mlp.fc2.bias", "student.module.blocks.3.norm1.weight", "student.module.blocks.3.norm1.bias", "student.module.blocks.3.attn.qkv.weight", "student.module.blocks.3.attn.qkv.bias", "student.module.blocks.3.attn.proj.weight", "student.module.blocks.3.attn.proj.bias", "student.module.blocks.3.norm2.weight", "student.module.blocks.3.norm2.bias", "student.module.blocks.3.mlp.fc1.weight", "student.module.blocks.3.mlp.fc1.bias", "student.module.blocks.3.mlp.fc2.weight", "student.module.blocks.3.mlp.fc2.bias", "student.module.blocks.4.norm1.weight", "student.module.blocks.4.norm1.bias", "student.module.blocks.4.attn.qkv.weight", "student.module.blocks.4.attn.qkv.bias", "student.module.blocks.4.attn.proj.weight", "student.module.blocks.4.attn.proj.bias", "student.module.blocks.4.norm2.weight", "student.module.blocks.4.norm2.bias", "student.module.blocks.4.mlp.fc1.weight", "student.module.blocks.4.mlp.fc1.bias", "student.module.blocks.4.mlp.fc2.weight", "student.module.blocks.4.mlp.fc2.bias", "student.module.blocks.5.norm1.weight", "student.module.blocks.5.norm1.bias", "student.module.blocks.5.attn.qkv.weight", "student.module.blocks.5.attn.qkv.bias", "student.module.blocks.5.attn.proj.weight", "student.module.blocks.5.attn.proj.bias", "student.module.blocks.5.norm2.weight", "student.module.blocks.5.norm2.bias", "student.module.blocks.5.mlp.fc1.weight", "student.module.blocks.5.mlp.fc1.bias", "student.module.blocks.5.mlp.fc2.weight", "student.module.blocks.5.mlp.fc2.bias", "student.module.blocks.6.norm1.weight", "student.module.blocks.6.norm1.bias", "student.module.blocks.6.attn.qkv.weight", "student.module.blocks.6.attn.qkv.bias", "student.module.blocks.6.attn.proj.weight", "student.module.blocks.6.attn.proj.bias", "student.module.blocks.6.norm2.weight", "student.module.blocks.6.norm2.bias", "student.module.blocks.6.mlp.fc1.weight", "student.module.blocks.6.mlp.fc1.bias", "student.module.blocks.6.mlp.fc2.weight", "student.module.blocks.6.mlp.fc2.bias", "student.module.blocks.7.norm1.weight", "student.module.blocks.7.norm1.bias", "student.module.blocks.7.attn.qkv.weight", "student.module.blocks.7.attn.qkv.bias", "student.module.blocks.7.attn.proj.weight", "student.module.blocks.7.attn.proj.bias", "student.module.blocks.7.norm2.weight", "student.module.blocks.7.norm2.bias", "student.module.blocks.7.mlp.fc1.weight", "student.module.blocks.7.mlp.fc1.bias", "student.module.blocks.7.mlp.fc2.weight", "student.module.blocks.7.mlp.fc2.bias", "student.module.blocks.8.norm1.weight", "student.module.blocks.8.norm1.bias", "student.module.blocks.8.attn.qkv.weight", "student.module.blocks.8.attn.qkv.bias", "student.module.blocks.8.attn.proj.weight", "student.module.blocks.8.attn.proj.bias", "student.module.blocks.8.norm2.weight", "student.module.blocks.8.norm2.bias", "student.module.blocks.8.mlp.fc1.weight", "student.module.blocks.8.mlp.fc1.bias", "student.module.blocks.8.mlp.fc2.weight", "student.module.blocks.8.mlp.fc2.bias", "student.module.blocks.9.norm1.weight", "student.module.blocks.9.norm1.bias", "student.module.blocks.9.attn.qkv.weight", "student.module.blocks.9.attn.qkv.bias", "student.module.blocks.9.attn.proj.weight", "student.module.blocks.9.attn.proj.bias", "student.module.blocks.9.norm2.weight", "student.module.blocks.9.norm2.bias", "student.module.blocks.9.mlp.fc1.weight", "student.module.blocks.9.mlp.fc1.bias", "student.module.blocks.9.mlp.fc2.weight", "student.module.blocks.9.mlp.fc2.bias", "student.module.blocks.10.norm1.weight", "student.module.blocks.10.norm1.bias", "student.module.blocks.10.attn.qkv.weight", "student.module.blocks.10.attn.qkv.bias", "student.module.blocks.10.attn.proj.weight", "student.module.blocks.10.attn.proj.bias", "student.module.blocks.10.norm2.weight", "student.module.blocks.10.norm2.bias", "student.module.blocks.10.mlp.fc1.weight", "student.module.blocks.10.mlp.fc1.bias", "student.module.blocks.10.mlp.fc2.weight", "student.module.blocks.10.mlp.fc2.bias", "student.module.blocks.11.norm1.weight", "student.module.blocks.11.norm1.bias", "student.module.blocks.11.attn.qkv.weight", "student.module.blocks.11.attn.qkv.bias", "student.module.blocks.11.attn.proj.weight", "student.module.blocks.11.attn.proj.bias", "student.module.blocks.11.norm2.weight", "student.module.blocks.11.norm2.bias", "student.module.blocks.11.mlp.fc1.weight", "student.module.blocks.11.mlp.fc1.bias", "student.module.blocks.11.mlp.fc2.weight", "student.module.blocks.11.mlp.fc2.bias", "student.module.norm.weight", "student.module.norm.bias", "student.module.head.weight", "student.module.head.bias". 
	Unexpected key(s) in state_dict: "student.cls_token", "student.pos_embed", "student.patch_embed.proj.weight", "student.patch_embed.proj.bias", "student.blocks.0.norm1.weight", "student.blocks.0.norm1.bias", "student.blocks.0.attn.qkv.weight", "student.blocks.0.attn.qkv.bias", "student.blocks.0.attn.proj.weight", "student.blocks.0.attn.proj.bias", "student.blocks.0.norm2.weight", "student.blocks.0.norm2.bias", "student.blocks.0.mlp.fc1.weight", "student.blocks.0.mlp.fc1.bias", "student.blocks.0.mlp.fc2.weight", "student.blocks.0.mlp.fc2.bias", "student.blocks.1.norm1.weight", "student.blocks.1.norm1.bias", "student.blocks.1.attn.qkv.weight", "student.blocks.1.attn.qkv.bias", "student.blocks.1.attn.proj.weight", "student.blocks.1.attn.proj.bias", "student.blocks.1.norm2.weight", "student.blocks.1.norm2.bias", "student.blocks.1.mlp.fc1.weight", "student.blocks.1.mlp.fc1.bias", "student.blocks.1.mlp.fc2.weight", "student.blocks.1.mlp.fc2.bias", "student.blocks.2.norm1.weight", "student.blocks.2.norm1.bias", "student.blocks.2.attn.qkv.weight", "student.blocks.2.attn.qkv.bias", "student.blocks.2.attn.proj.weight", "student.blocks.2.attn.proj.bias", "student.blocks.2.norm2.weight", "student.blocks.2.norm2.bias", "student.blocks.2.mlp.fc1.weight", "student.blocks.2.mlp.fc1.bias", "student.blocks.2.mlp.fc2.weight", "student.blocks.2.mlp.fc2.bias", "student.blocks.3.norm1.weight", "student.blocks.3.norm1.bias", "student.blocks.3.attn.qkv.weight", "student.blocks.3.attn.qkv.bias", "student.blocks.3.attn.proj.weight", "student.blocks.3.attn.proj.bias", "student.blocks.3.norm2.weight", "student.blocks.3.norm2.bias", "student.blocks.3.mlp.fc1.weight", "student.blocks.3.mlp.fc1.bias", "student.blocks.3.mlp.fc2.weight", "student.blocks.3.mlp.fc2.bias", "student.blocks.4.norm1.weight", "student.blocks.4.norm1.bias", "student.blocks.4.attn.qkv.weight", "student.blocks.4.attn.qkv.bias", "student.blocks.4.attn.proj.weight", "student.blocks.4.attn.proj.bias", "student.blocks.4.norm2.weight", "student.blocks.4.norm2.bias", "student.blocks.4.mlp.fc1.weight", "student.blocks.4.mlp.fc1.bias", "student.blocks.4.mlp.fc2.weight", "student.blocks.4.mlp.fc2.bias", "student.blocks.5.norm1.weight", "student.blocks.5.norm1.bias", "student.blocks.5.attn.qkv.weight", "student.blocks.5.attn.qkv.bias", "student.blocks.5.attn.proj.weight", "student.blocks.5.attn.proj.bias", "student.blocks.5.norm2.weight", "student.blocks.5.norm2.bias", "student.blocks.5.mlp.fc1.weight", "student.blocks.5.mlp.fc1.bias", "student.blocks.5.mlp.fc2.weight", "student.blocks.5.mlp.fc2.bias", "student.blocks.6.norm1.weight", "student.blocks.6.norm1.bias", "student.blocks.6.attn.qkv.weight", "student.blocks.6.attn.qkv.bias", "student.blocks.6.attn.proj.weight", "student.blocks.6.attn.proj.bias", "student.blocks.6.norm2.weight", "student.blocks.6.norm2.bias", "student.blocks.6.mlp.fc1.weight", "student.blocks.6.mlp.fc1.bias", "student.blocks.6.mlp.fc2.weight", "student.blocks.6.mlp.fc2.bias", "student.blocks.7.norm1.weight", "student.blocks.7.norm1.bias", "student.blocks.7.attn.qkv.weight", "student.blocks.7.attn.qkv.bias", "student.blocks.7.attn.proj.weight", "student.blocks.7.attn.proj.bias", "student.blocks.7.norm2.weight", "student.blocks.7.norm2.bias", "student.blocks.7.mlp.fc1.weight", "student.blocks.7.mlp.fc1.bias", "student.blocks.7.mlp.fc2.weight", "student.blocks.7.mlp.fc2.bias", "student.blocks.8.norm1.weight", "student.blocks.8.norm1.bias", "student.blocks.8.attn.qkv.weight", "student.blocks.8.attn.qkv.bias", "student.blocks.8.attn.proj.weight", "student.blocks.8.attn.proj.bias", "student.blocks.8.norm2.weight", "student.blocks.8.norm2.bias", "student.blocks.8.mlp.fc1.weight", "student.blocks.8.mlp.fc1.bias", "student.blocks.8.mlp.fc2.weight", "student.blocks.8.mlp.fc2.bias", "student.blocks.9.norm1.weight", "student.blocks.9.norm1.bias", "student.blocks.9.attn.qkv.weight", "student.blocks.9.attn.qkv.bias", "student.blocks.9.attn.proj.weight", "student.blocks.9.attn.proj.bias", "student.blocks.9.norm2.weight", "student.blocks.9.norm2.bias", "student.blocks.9.mlp.fc1.weight", "student.blocks.9.mlp.fc1.bias", "student.blocks.9.mlp.fc2.weight", "student.blocks.9.mlp.fc2.bias", "student.blocks.10.norm1.weight", "student.blocks.10.norm1.bias", "student.blocks.10.attn.qkv.weight", "student.blocks.10.attn.qkv.bias", "student.blocks.10.attn.proj.weight", "student.blocks.10.attn.proj.bias", "student.blocks.10.norm2.weight", "student.blocks.10.norm2.bias", "student.blocks.10.mlp.fc1.weight", "student.blocks.10.mlp.fc1.bias", "student.blocks.10.mlp.fc2.weight", "student.blocks.10.mlp.fc2.bias", "student.blocks.11.norm1.weight", "student.blocks.11.norm1.bias", "student.blocks.11.attn.qkv.weight", "student.blocks.11.attn.qkv.bias", "student.blocks.11.attn.proj.weight", "student.blocks.11.attn.proj.bias", "student.blocks.11.norm2.weight", "student.blocks.11.norm2.bias", "student.blocks.11.mlp.fc1.weight", "student.blocks.11.mlp.fc1.bias", "student.blocks.11.mlp.fc2.weight", "student.blocks.11.mlp.fc2.bias", "student.norm.weight", "student.norm.bias", "student.head.weight", "student.head.bias". 
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Base model's results
Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 302, in <module>
    acc, ent = accuracy_ent(algorithm, loader, weights, device, adapt=None)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 95, in accuracy_ent
    p = network(x)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 201, in _forward_unimplemented
    raise NotImplementedError
NotImplementedError
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Base model's results
Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 302, in <module>
    acc, ent = accuracy_ent(algorithm, loader, weights, device, adapt=None)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 95, in accuracy_ent
    p = network(x)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 201, in _forward_unimplemented
    raise NotImplementedError
NotImplementedError
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Base model's results
> /local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py(96)accuracy_ent()
-> p = network(x)
(Pdb) (Pdb) ERM_ViT(
  (student): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
    (head): Linear(in_features=384, out_features=5, bias=True)
  )
)
(Pdb) *** NotImplementedError
(Pdb) *** NotImplementedError
(Pdb) ['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_backward_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_update_ema_variables', '_version', 'add_module', 'apply', 'bfloat16', 'buffers', 'children', 'cpu', 'cuda', 'cutmix', 'double', 'dump_patches', 'ema', 'eval', 'extra_repr', 'float', 'forward', 'half', 'hparams', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'optimizer', 'parameters', 'predict', 'predict_Train', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_parameter', 'requires_grad_', 'share_memory', 'state_dict', 'student', 'to', 'train', 'training', 'type', 'update', 'xpu', 'zero_grad']
(Pdb) [tensor([[ 0.0454, -0.3911, -0.7847, -0.1743,  0.3332],
        [ 0.0613, -0.3683, -0.8004, -0.1691,  0.3472],
        [ 0.0802, -0.3629, -0.7911, -0.1615,  0.2901],
        [ 0.0908, -0.3864, -0.7930, -0.1370,  0.2909],
        [ 0.0757, -0.3961, -0.7572, -0.1489,  0.3490],
        [ 0.0717, -0.3844, -0.7884, -0.1669,  0.3254],
        [ 0.0752, -0.4200, -0.7759, -0.1644,  0.3150],
        [ 0.0753, -0.3703, -0.8102, -0.2125,  0.2953],
        [ 0.0983, -0.3667, -0.7951, -0.1375,  0.3545],
        [ 0.0774, -0.3904, -0.7893, -0.1198,  0.3161],
        [ 0.0754, -0.3723, -0.7962, -0.1797,  0.3086],
        [ 0.0665, -0.3790, -0.8157, -0.2041,  0.2981],
        [ 0.0712, -0.3929, -0.7604, -0.1068,  0.3785],
        [ 0.0635, -0.3744, -0.7541, -0.1809,  0.3345],
        [ 0.1161, -0.3779, -0.8171, -0.1628,  0.3320],
        [ 0.0799, -0.4049, -0.8055, -0.1756,  0.3260],
        [ 0.0926, -0.3937, -0.7643, -0.1291,  0.3261],
        [ 0.0741, -0.3680, -0.7913, -0.0883,  0.3679],
        [ 0.0912, -0.3789, -0.7916, -0.2167,  0.2913],
        [ 0.0713, -0.3717, -0.8026, -0.1310,  0.2922],
        [ 0.0597, -0.3799, -0.8067, -0.2176,  0.3057],
        [ 0.0842, -0.4030, -0.7990, -0.1138,  0.3282],
        [ 0.0724, -0.3551, -0.7721, -0.1843,  0.3307],
        [ 0.0936, -0.4074, -0.7489, -0.1740,  0.3156],
        [ 0.0841, -0.3914, -0.7987, -0.1632,  0.3286],
        [ 0.1063, -0.3941, -0.7975, -0.1575,  0.2986],
        [ 0.0668, -0.3911, -0.7854, -0.0923,  0.3083],
        [ 0.0860, -0.3704, -0.8283, -0.1535,  0.3108],
        [ 0.0917, -0.3786, -0.7948, -0.1207,  0.3381],
        [ 0.0596, -0.4063, -0.7659, -0.1666,  0.3350],
        [ 0.0844, -0.3657, -0.8023, -0.1855,  0.3275],
        [ 0.0710, -0.4037, -0.7642, -0.1345,  0.3329]], device='cuda:0'), tensor([[ 0.2276, -0.3045, -0.3873, -0.1096,  0.2376],
        [ 0.2414, -0.2647, -0.4219, -0.1208,  0.2519],
        [ 0.2489, -0.3286, -0.4875, -0.0960,  0.2149],
        [ 0.2419, -0.2684, -0.4728, -0.1162,  0.2127],
        [ 0.2483, -0.2994, -0.4039, -0.0951,  0.2330],
        [ 0.2586, -0.2963, -0.4551, -0.1248,  0.1824],
        [ 0.2496, -0.3220, -0.3646, -0.1072,  0.2043],
        [ 0.2576, -0.2518, -0.4008, -0.1360,  0.1553],
        [ 0.2606, -0.2663, -0.4638, -0.0868,  0.2672],
        [ 0.2282, -0.2620, -0.4583, -0.0853,  0.2253],
        [ 0.2502, -0.2570, -0.3913, -0.1404,  0.2086],
        [ 0.2626, -0.2792, -0.3529, -0.1382,  0.2018],
        [ 0.2170, -0.3210, -0.4925, -0.0785,  0.2650],
        [ 0.2479, -0.2835, -0.4512, -0.1279,  0.1850],
        [ 0.3144, -0.2841, -0.5012, -0.0953,  0.2437],
        [ 0.2652, -0.3101, -0.3794, -0.1223,  0.2447],
        [ 0.2622, -0.2810, -0.4858, -0.0915,  0.2114],
        [ 0.1853, -0.2436, -0.4588, -0.0428,  0.3035],
        [ 0.2607, -0.2665, -0.3878, -0.1291,  0.1610],
        [ 0.2256, -0.2826, -0.5264, -0.0969,  0.2279],
        [ 0.2720, -0.2698, -0.3346, -0.1256,  0.1936],
        [ 0.2208, -0.2654, -0.4723, -0.1043,  0.2292],
        [ 0.2667, -0.2947, -0.4207, -0.0927,  0.2797],
        [ 0.2574, -0.3136, -0.3957, -0.1198,  0.1528],
        [ 0.2571, -0.2946, -0.4137, -0.1225,  0.2202],
        [ 0.2510, -0.2626, -0.4649, -0.1154,  0.2147],
        [ 0.1900, -0.2808, -0.4809, -0.0778,  0.2213],
        [ 0.2647, -0.2993, -0.4906, -0.1122,  0.2271],
        [ 0.2367, -0.2768, -0.5118, -0.0958,  0.2645],
        [ 0.2635, -0.2980, -0.4205, -0.1360,  0.1988],
        [ 0.2278, -0.2746, -0.4428, -0.1253,  0.2087],
        [ 0.2326, -0.2781, -0.4423, -0.1126,  0.2470]], device='cuda:0'), tensor([[ 0.2205, -0.2897, -0.2529, -0.1091,  0.2649],
        [ 0.2566, -0.2789, -0.2933, -0.0981,  0.2765],
        [ 0.2100, -0.2924, -0.3304, -0.0988,  0.2887],
        [ 0.2181, -0.2031, -0.3005, -0.1318,  0.2621],
        [ 0.2086, -0.2940, -0.2451, -0.1068,  0.2043],
        [ 0.2418, -0.2241, -0.2893, -0.1116,  0.1607],
        [ 0.2398, -0.2968, -0.1907, -0.1093,  0.2423],
        [ 0.2596, -0.2268, -0.2381, -0.2110,  0.1976],
        [ 0.2217, -0.2308, -0.2863, -0.0954,  0.3155],
        [ 0.2114, -0.2152, -0.3096, -0.0633,  0.2719],
        [ 0.2298, -0.2202, -0.2398, -0.1905,  0.1893],
        [ 0.2624, -0.2846, -0.2134, -0.1550,  0.2323],
        [ 0.2562, -0.2796, -0.3332, -0.0866,  0.3299],
        [ 0.2187, -0.2419, -0.3374, -0.2014,  0.2392],
        [ 0.3151, -0.2281, -0.3089, -0.0977,  0.2529],
        [ 0.2326, -0.2937, -0.2469, -0.1293,  0.2576],
        [ 0.2364, -0.2176, -0.2920, -0.0399,  0.1939],
        [ 0.1518, -0.1827, -0.2720, -0.0029,  0.3945],
        [ 0.2409, -0.2441, -0.2651, -0.1817,  0.1641],
        [ 0.2686, -0.2089, -0.3623, -0.1023,  0.3282],
        [ 0.3464, -0.1885, -0.1962, -0.1510,  0.2445],
        [ 0.1925, -0.2183, -0.3032, -0.1039,  0.2811],
        [ 0.2869, -0.2654, -0.2931, -0.1204,  0.3728],
        [ 0.2143, -0.2619, -0.2242, -0.1629,  0.1642],
        [ 0.2108, -0.2570, -0.2704, -0.1263,  0.2144],
        [ 0.1934, -0.2743, -0.3114, -0.1147,  0.2460],
        [ 0.1835, -0.2137, -0.2849, -0.0881,  0.2990],
        [ 0.2564, -0.2841, -0.3529, -0.1462,  0.2443],
        [ 0.2100, -0.2128, -0.3262, -0.0937,  0.3018],
        [ 0.2236, -0.2264, -0.2495, -0.1860,  0.2049],
        [ 0.2064, -0.2705, -0.2767, -0.1283,  0.2550],
        [ 0.2107, -0.2505, -0.3050, -0.1078,  0.2574]], device='cuda:0'), tensor([[ 0.2121, -0.2451, -0.1541, -0.1404,  0.0882],
        [ 0.2008, -0.1961, -0.1991, -0.1437,  0.1375],
        [ 0.2544, -0.2371, -0.1415, -0.1574,  0.1012],
        [ 0.2102, -0.2030, -0.1651, -0.1174,  0.1304],
        [ 0.2299, -0.2777, -0.1002, -0.1455,  0.0497],
        [ 0.2537, -0.2365, -0.1933, -0.1628,  0.0254],
        [ 0.2497, -0.2472, -0.0718, -0.1476,  0.0714],
        [ 0.2974, -0.1545, -0.0525, -0.1726,  0.0118],
        [ 0.2403, -0.2044, -0.1874, -0.1946,  0.1705],
        [ 0.1972, -0.2015, -0.1958, -0.0826,  0.1242],
        [ 0.2303, -0.1861, -0.0850, -0.1606,  0.0113],
        [ 0.2773, -0.1843, -0.0124, -0.1754,  0.1474],
        [ 0.2425, -0.2667, -0.2864, -0.1652,  0.0906],
        [ 0.2406, -0.1943, -0.2468, -0.2205, -0.0260],
        [ 0.3142, -0.1801, -0.2079, -0.1423,  0.0512],
        [ 0.2234, -0.2155, -0.1349, -0.0864,  0.1217],
        [ 0.2367, -0.2265, -0.2172, -0.0830,  0.1229],
        [ 0.0653, -0.0988, -0.1681, -0.1052,  0.2410],
        [ 0.2414, -0.1535, -0.1038, -0.1808,  0.0227],
        [ 0.2959, -0.2706, -0.2313, -0.1022,  0.1304],
        [ 0.2174, -0.0947,  0.0101, -0.0695,  0.0853],
        [ 0.2014, -0.1940, -0.1918, -0.0783,  0.1360],
        [ 0.2434, -0.1691, -0.1935, -0.0508,  0.2044],
        [ 0.2391, -0.2451, -0.0716, -0.1872,  0.0246],
        [ 0.2334, -0.1878, -0.1299, -0.1384,  0.0681],
        [ 0.2380, -0.2215, -0.1471, -0.2020,  0.1412],
        [ 0.1931, -0.2619, -0.0881, -0.0830,  0.1319],
        [ 0.2365, -0.2559, -0.2202, -0.0727,  0.0494],
        [ 0.2497, -0.2435, -0.2269, -0.1376,  0.1703],
        [ 0.2620, -0.2732, -0.1759, -0.2213, -0.0400],
        [ 0.2543, -0.2000, -0.1336, -0.2029,  0.1170],
        [ 0.1407, -0.1800, -0.2505, -0.0803,  0.0958]], device='cuda:0'), tensor([[ 0.1004, -0.3240, -0.1459, -0.0274, -0.0616],
        [ 0.1427, -0.3060, -0.1515,  0.0250, -0.0138],
        [ 0.2192, -0.2851, -0.1582,  0.0810, -0.1040],
        [ 0.1660, -0.2074, -0.2506,  0.1571, -0.0349],
        [ 0.1109, -0.3536, -0.1102,  0.0553, -0.1441],
        [ 0.1255, -0.3142, -0.2145,  0.0993, -0.1091],
        [ 0.0962, -0.2803, -0.0746,  0.0322, -0.1805],
        [ 0.1277, -0.2368, -0.0779,  0.1752, -0.2304],
        [ 0.1480, -0.3373, -0.1911, -0.0119,  0.0199],
        [ 0.1348, -0.2984, -0.1930,  0.0950,  0.0015],
        [ 0.1277, -0.2072, -0.1824,  0.0865, -0.1062],
        [ 0.1706, -0.2514,  0.0066,  0.0929, -0.0820],
        [ 0.2508, -0.2141, -0.2761,  0.1641, -0.0544],
        [ 0.1855, -0.2049, -0.1912,  0.0103, -0.1098],
        [ 0.1388, -0.2983, -0.1970,  0.0259, -0.0744],
        [ 0.1028, -0.2536, -0.1215,  0.0982,  0.0093],
        [ 0.1291, -0.3060, -0.2160,  0.1166, -0.1225],
        [-0.0376, -0.2301, -0.1700,  0.1782, -0.0118],
        [ 0.1401, -0.2958, -0.1018,  0.0279, -0.1245],
        [ 0.2653, -0.2767, -0.2498,  0.1844, -0.0088],
        [ 0.0629, -0.2300, -0.0045,  0.1626, -0.0717],
        [ 0.1502, -0.2169, -0.2462,  0.1510,  0.0175],
        [ 0.1543, -0.2768, -0.1382,  0.1757,  0.0497],
        [ 0.0704, -0.3801, -0.0988,  0.0085, -0.1304],
        [ 0.0813, -0.2218, -0.1395,  0.0548, -0.1437],
        [ 0.2225, -0.2908, -0.1652,  0.0055, -0.0640],
        [ 0.1310, -0.2702, -0.2139,  0.1320, -0.0186],
        [ 0.1156, -0.2296, -0.2586,  0.0687,  0.0141],
        [ 0.1179, -0.3975, -0.2171,  0.0657, -0.0298],
        [ 0.2422, -0.3384, -0.1685,  0.0340, -0.1663],
        [ 0.1905, -0.2219, -0.1063, -0.0103, -0.0837],
        [ 0.1421, -0.2382, -0.2356,  0.1255, -0.0732]], device='cuda:0'), tensor([[ 3.3572e-01, -4.7572e-01, -2.1091e-01, -1.1045e-01,  6.7910e-02],
        [ 3.3618e-01, -3.7308e-01, -3.0172e-01, -2.0208e-01,  1.0163e-01],
        [ 3.9642e-01, -4.5558e-01, -3.1676e-01, -4.5169e-02, -3.1593e-02],
        [ 2.8078e-01, -2.4554e-01, -3.0451e-01,  1.4871e-01, -1.3597e-01],
        [ 2.7770e-01, -4.9322e-01, -2.0588e-01, -5.2398e-02, -2.5953e-01],
        [ 2.8448e-01, -4.5652e-01, -2.6279e-01,  1.0859e-02, -1.1833e-01],
        [ 2.3010e-01, -4.4404e-01, -2.2515e-01, -6.7560e-02, -1.7302e-01],
        [ 1.1539e-01, -1.8202e-01, -7.2121e-02, -2.5843e-02, -2.7755e-01],
        [ 4.2202e-01, -6.0245e-01, -2.6434e-01, -1.6341e-01,  1.1999e-01],
        [ 3.0581e-01, -4.8557e-01, -3.4198e-01, -3.0729e-02,  2.4514e-02],
        [ 2.3649e-01, -2.8996e-01, -2.6949e-01, -1.5783e-04, -1.5528e-01],
        [ 3.8279e-01, -3.6247e-01, -6.3453e-02,  1.6276e-02, -1.1565e-01],
        [ 4.7931e-01, -3.3027e-01, -2.9294e-01,  8.3232e-02,  9.6010e-03],
        [ 2.2150e-01, -3.5334e-01, -2.5623e-01,  5.2350e-03,  4.2128e-02],
        [ 3.6188e-01, -5.6102e-01, -3.3846e-01, -6.2780e-02,  6.9401e-03],
        [ 2.7156e-01, -3.8865e-01, -2.9899e-01,  1.9447e-02,  1.0463e-02],
        [ 3.0273e-01, -4.6941e-01, -3.4554e-01,  2.3532e-02, -8.2369e-02],
        [ 1.2532e-01, -4.6306e-01, -2.7185e-01,  1.6136e-01, -3.3531e-02],
        [ 2.5770e-01, -3.9244e-01, -2.2938e-01, -2.2766e-02, -1.2823e-01],
        [ 3.2905e-01, -2.6429e-01, -2.8839e-01,  1.7685e-01, -8.4881e-02],
        [ 1.2983e-01, -3.0577e-01, -1.6115e-02,  2.0838e-02,  4.2545e-02],
        [ 2.2803e-01, -3.4347e-01, -3.2166e-01,  1.5623e-01,  4.6741e-03],
        [ 2.7628e-01, -4.5712e-01, -1.9300e-01,  7.1138e-02,  1.0873e-01],
        [ 2.6913e-01, -5.7947e-01, -1.6183e-01, -1.3614e-01, -1.6467e-01],
        [ 2.8589e-01, -3.7365e-01, -2.8675e-01, -1.4736e-02, -1.2998e-01],
        [ 5.0153e-01, -4.1517e-01, -2.2234e-01, -7.2441e-02, -8.1333e-02],
        [ 2.6354e-01, -3.5478e-01, -4.3122e-01,  1.6140e-01, -1.7268e-02],
        [ 2.7425e-01, -3.3498e-01, -3.9659e-01,  3.3738e-02,  9.3686e-02],
        [ 3.6284e-01, -5.5013e-01, -3.3883e-01, -8.1013e-02,  2.8390e-02],
        [ 2.0082e-01, -5.4585e-01, -2.5637e-01,  6.1273e-02, -1.0537e-01],
        [ 3.7416e-01, -3.8426e-01, -2.7959e-01, -5.1875e-02, -1.0538e-01],
        [ 2.4129e-01, -4.0191e-01, -2.1715e-01, -3.4368e-02, -3.8086e-02]],
       device='cuda:0'), tensor([[ 0.3578, -0.6797, -0.2420, -0.3652,  0.1165],
        [ 0.3287, -0.5299, -0.3032, -0.3779,  0.2447],
        [ 0.5714, -0.7756, -0.5618, -0.1691,  0.1604],
        [ 0.3629, -0.1691, -0.2179, -0.0581, -0.2282],
        [ 0.2157, -0.8899, -0.0851, -0.0683, -0.1234],
        [ 0.4010, -0.5856, -0.2065, -0.1677,  0.2401],
        [ 0.3290, -0.9311, -0.4350, -0.3230, -0.0145],
        [ 0.1498, -0.2422, -0.0630, -0.0441, -0.3618],
        [ 0.5252, -0.7712, -0.2643, -0.3251,  0.2392],
        [ 0.2577, -0.8980, -0.2520, -0.1746,  0.1049],
        [ 0.0567, -0.2802, -0.1277, -0.0992, -0.2112],
        [ 0.3976, -0.9231, -0.2362,  0.0127,  0.0951],
        [ 0.5392, -0.0827, -0.1561, -0.0595,  0.1962],
        [ 0.2580, -0.1379, -0.2186, -0.0865,  0.2842],
        [ 0.4311, -0.9260, -0.3024, -0.1265,  0.1524],
        [ 0.4612, -0.6219, -0.4384, -0.1158,  0.3133],
        [ 0.2824, -0.9084, -0.3713, -0.1233,  0.1715],
        [ 0.0977, -0.5626, -0.0112, -0.1569, -0.3134],
        [ 0.3524, -0.3620, -0.2031, -0.1168, -0.2980],
        [ 0.2276, -0.1951, -0.0502, -0.1684, -0.1578],
        [ 0.2601, -0.2827,  0.1093, -0.1487, -0.0816],
        [ 0.1628, -0.2703, -0.2082, -0.0204, -0.1485],
        [ 0.3991, -0.4552, -0.0687, -0.0080,  0.0845],
        [ 0.2613, -0.7950, -0.2548, -0.4442,  0.2025],
        [ 0.4074, -0.8576, -0.3263, -0.2275,  0.1342],
        [ 0.5435, -0.8995, -0.3253, -0.0745,  0.1217],
        [ 0.0910, -0.2255, -0.2339,  0.0305, -0.1510],
        [ 0.3135, -0.3797, -0.3255,  0.1020,  0.0759],
        [ 0.5298, -0.9604, -0.3164, -0.1998,  0.3992],
        [ 0.2166, -0.4731, -0.2459, -0.1059, -0.1973],
        [ 0.5890, -0.7599, -0.4017, -0.2331,  0.3208],
        [ 0.4520, -0.2174, -0.1214, -0.1808, -0.0646]], device='cuda:0'), tensor([[ 0.2425, -0.8575, -0.3301, -0.3236,  0.4370],
        [ 0.5477, -0.4782, -0.4314, -0.4022,  0.7085],
        [ 0.3100, -0.8136, -0.7942, -0.0454,  0.5961],
        [ 0.2391, -0.0701, -0.5845, -0.4553, -0.2419],
        [ 0.2082, -1.0977, -0.0478,  0.2424,  0.3371],
        [ 0.4956, -0.7099, -0.5126, -0.2217,  0.7256],
        [ 0.1919, -1.1542, -0.5928,  0.0125,  0.5920],
        [ 0.2997, -0.6578,  0.5456,  0.3529,  0.0891],
        [ 0.4182, -1.0459, -0.1035, -0.0148,  0.8223],
        [ 0.2549, -1.2620, -0.1768,  0.0699,  0.6600],
        [-0.1745,  0.2266, -0.6999, -0.5187, -0.3065],
        [ 0.3139, -1.2853, -0.4312,  0.3271,  0.4244],
        [ 0.9211, -0.2105, -0.2724, -0.3932,  0.2343],
        [ 0.4544, -0.2258, -0.3922, -0.1596,  0.3818],
        [ 0.2526, -1.1383, -0.4447, -0.1212,  0.7366],
        [ 0.4015, -0.7994, -0.7163, -0.0526,  0.6946],
        [ 0.3303, -1.0443, -0.3970,  0.1113,  0.6116],
        [ 0.1237, -0.5870,  0.2959, -0.1717, -0.2807],
        [ 0.8585, -0.3332, -0.3781, -0.2746, -0.0357],
        [ 0.8392, -0.2765, -0.0511, -0.4110,  0.0923],
        [ 0.6357, -0.6123,  0.3828, -0.0126, -0.0751],
        [-0.0598, -0.1007, -0.4618, -0.4005,  0.0234],
        [ 0.7276, -0.4631,  0.0906,  0.1473,  0.1348],
        [ 0.2730, -0.9556, -0.5208, -0.4066,  0.7075],
        [ 0.4326, -0.8518, -0.5013, -0.0484,  0.5715],
        [ 0.5851, -0.9300, -0.6270,  0.2433,  0.6565],
        [-0.0954, -0.0206, -0.6280, -0.2557, -0.0873],
        [ 0.4001, -0.2736, -0.1885, -0.1042,  0.4737],
        [ 0.5295, -1.1861, -0.5086,  0.0763,  0.8878],
        [ 0.4860, -0.2866, -0.4891, -0.6159,  0.0138],
        [ 0.5603, -0.9373, -0.7272, -0.0353,  0.6752],
        [ 1.1382, -0.0596, -0.2357, -0.2735,  0.4719]], device='cuda:0'), tensor([[ 0.1565, -1.4027, -0.2460, -0.2290,  1.1848],
        [ 0.8206, -0.7461, -0.6265, -0.3378,  1.2342],
        [-0.2401, -1.2549, -0.9270,  0.1300,  1.6746],
        [ 0.2253, -0.1357, -0.5621, -0.8542,  0.5205],
        [-0.0417, -1.5346, -0.4960,  0.3793,  1.3420],
        [ 0.2115, -1.1401, -0.5874,  0.2852,  1.7832],
        [-0.1377, -1.4846, -1.0042,  0.1810,  1.6649],
        [ 0.2234, -1.1620,  1.1910,  0.0175, -0.2214],
        [-0.1536, -1.4905, -0.2932,  0.2920,  1.5932],
        [-0.3768, -1.9353, -0.4259,  0.2680,  1.4126],
        [-0.1496,  0.6836, -0.5574, -1.0897,  0.2180],
        [-0.2989, -1.6638, -0.9326,  0.4627,  1.5501],
        [ 1.4610, -0.7097, -0.4658, -0.3768, -0.0124],
        [ 1.2270, -0.7835, -0.5098, -0.0331,  0.7327],
        [-0.1450, -1.6704, -0.4122,  0.2257,  1.6691],
        [ 0.1378, -1.2359, -0.8868,  0.0411,  1.5966],
        [-0.1983, -1.5671, -0.6966,  0.2111,  1.4898],
        [ 0.1125, -1.2471,  0.8170, -0.2350, -0.3783],
        [ 1.4355, -0.5392, -0.1457,  0.0522, -0.0497],
        [ 0.8746, -0.6083, -0.3401, -0.4361,  0.2771],
        [ 1.2205, -0.8975,  0.5180,  0.0023,  0.1280],
        [ 0.0574,  0.2838, -0.0236, -0.9731,  0.4489],
        [ 1.2035, -0.6780,  0.1822,  0.0089, -0.3759],
        [-0.2256, -1.4525, -0.7814,  0.0638,  1.8926],
        [-0.0196, -1.3495, -0.6718,  0.2264,  1.6677],
        [ 0.2247, -1.4265, -0.7622,  0.2739,  1.6391],
        [-0.0446, -0.2820, -0.6784, -0.5643,  0.6272],
        [ 0.2777, -1.0314,  0.1822,  0.0049,  0.4174],
        [-0.1413, -1.6531, -0.5891,  0.2935,  1.9203],
        [ 1.0798, -0.8965, -0.6528, -0.4459,  0.2210],
        [ 0.0790, -1.3854, -0.9659,  0.3967,  1.6772],
        [ 1.6974, -0.7330, -0.0745, -0.2021,  0.6987]], device='cuda:0'), tensor([[-1.3419e-02, -1.7131e+00, -2.5450e-01, -7.4692e-01,  2.0925e+00],
        [ 9.4633e-01, -1.2388e+00, -8.0442e-01, -2.0324e-01,  1.8887e+00],
        [-5.5554e-01, -2.0799e+00, -1.1403e+00, -2.4463e-01,  2.7321e+00],
        [-9.0917e-02,  9.1454e-01, -6.7353e-01, -8.8722e-01,  7.2701e-01],
        [-1.0361e-01, -2.2299e+00, -4.9464e-01,  8.6591e-04,  2.5022e+00],
        [ 2.4644e-02, -1.4919e+00, -6.5336e-01, -1.6503e-01,  2.5526e+00],
        [-5.2631e-01, -2.3196e+00, -6.6476e-01,  5.9527e-02,  2.4341e+00],
        [ 3.5664e-01, -1.5583e+00,  1.7610e+00, -5.0702e-02,  1.9262e-01],
        [-6.6391e-01, -2.3531e+00,  1.0725e-01,  1.7167e-01,  2.4935e+00],
        [-7.5335e-01, -2.5298e+00, -8.5898e-02,  1.4975e-01,  2.1991e+00],
        [-6.6434e-01,  1.7406e+00, -4.7831e-01, -1.3532e+00,  4.8315e-01],
        [-6.4544e-01, -2.2023e+00, -1.1321e+00,  8.7563e-02,  2.3830e+00],
        [ 2.2608e+00, -6.7969e-01, -1.1790e+00, -3.3824e-01, -3.2632e-02],
        [ 1.6308e+00, -8.6451e-01, -9.6071e-01,  1.4908e-02,  3.5679e-01],
        [-8.3259e-01, -2.0149e+00,  2.9082e-01, -4.6125e-02,  2.4384e+00],
        [-2.6331e-01, -1.5997e+00, -1.0077e+00, -4.0574e-01,  2.5355e+00],
        [-2.1382e-01, -2.0848e+00, -8.3877e-01, -3.6391e-02,  2.7589e+00],
        [ 5.1965e-01, -1.6627e+00,  1.2310e+00, -3.5299e-01,  6.8919e-03],
        [ 2.0517e+00, -3.4633e-01, -6.0045e-01, -4.5256e-02, -2.0042e-01],
        [ 1.4471e+00, -8.0252e-01, -4.3545e-01, -4.1399e-01,  6.0889e-01],
        [ 1.7730e+00, -1.0745e+00,  2.1801e-01,  2.7642e-02,  5.6387e-01],
        [-3.1960e-01,  1.4529e+00, -3.7127e-01, -1.0947e+00,  6.2348e-01],
        [ 1.9150e+00, -5.8705e-01, -6.8077e-01, -3.6150e-01, -3.2140e-01],
        [-7.3323e-01, -2.1152e+00, -7.6409e-01, -4.1731e-01,  2.9892e+00],
        [-1.9996e-01, -1.7758e+00, -7.4051e-01, -2.7563e-02,  2.6049e+00],
        [ 5.1805e-02, -2.2082e+00, -8.1578e-01, -3.5693e-01,  2.5343e+00],
        [-6.7956e-01,  2.6488e-01, -7.5466e-01, -6.8114e-01,  1.0769e+00],
        [ 1.0846e-01, -1.4481e+00,  7.6604e-01, -3.4742e-01,  6.7220e-01],
        [-3.5944e-01, -2.1262e+00, -4.0192e-01, -1.3680e-01,  2.9208e+00],
        [ 1.6604e+00, -9.2377e-01, -1.3474e+00, -1.0482e+00, -2.2301e-01],
        [-3.4773e-01, -2.1275e+00, -1.1467e+00,  2.5577e-01,  2.7009e+00],
        [ 2.4443e+00, -7.1918e-01, -5.0896e-01,  3.4284e-02,  6.8871e-01]],
       device='cuda:0'), tensor([[-0.2810, -2.0167, -0.7563, -1.1792,  4.1915],
        [-0.0114, -1.4317, -0.8872, -0.1604,  2.9704],
        [-0.9885, -2.8219, -1.7616, -0.4039,  5.1459],
        [-1.2417,  3.7723, -1.2340, -1.4975, -0.9246],
        [-0.8404, -2.7346, -0.5382, -0.4089,  4.6084],
        [-0.6342, -1.9970, -0.9647, -0.5309,  5.0669],
        [-1.1971, -2.8516, -1.2073, -0.4288,  4.3909],
        [-0.1934, -1.7284,  3.8342, -0.7738, -0.9530],
        [-1.5080, -3.2839, -0.5533,  0.3828,  4.8497],
        [-1.7152, -3.5416, -1.0626,  0.3303,  4.6673],
        [-1.5707,  4.8018, -1.3281, -1.7828, -0.8463],
        [-0.9509, -2.9115, -1.6225, -0.0255,  4.2390],
        [ 3.9943, -0.5476, -0.7674, -0.0900, -1.0354],
        [ 4.1074, -0.7373, -0.2431, -0.2996, -0.0107],
        [-1.6152, -2.6543, -0.5548, -0.0979,  4.5614],
        [-0.7666, -1.9714, -1.3124, -0.0866,  4.7714],
        [-0.8064, -2.9736, -1.5731,  0.1961,  5.0374],
        [-0.1136, -2.0547,  3.2014, -1.1088, -0.1552],
        [ 4.0913, -0.4246,  0.7197, -0.5217, -0.9938],
        [ 2.6568, -0.7823,  0.0081, -0.7212, -0.1455],
        [ 3.0252, -1.0655,  0.6611, -0.0623, -0.0397],
        [-0.9461,  3.9748, -1.0360, -1.2187, -1.2217],
        [ 3.7139, -0.8959,  0.6216, -0.3662, -1.3975],
        [-1.3413, -3.2566, -1.7922, -0.3036,  5.7354],
        [-0.9268, -2.5057, -1.3102,  0.2652,  5.0858],
        [-0.1907, -2.7012, -1.1288, -0.5631,  4.2711],
        [-2.2706,  3.3012, -1.1080, -1.3015, -0.3303],
        [-1.0337, -1.7468,  1.9872, -1.2071,  0.4338],
        [-1.1956, -2.8005, -0.4932,  0.0305,  5.1280],
        [ 3.3662, -1.1881, -0.3072, -1.5461, -0.8995],
        [-0.8232, -2.7028, -1.8891,  0.3788,  5.1144],
        [ 4.6759, -0.5906,  0.2144, -0.2772,  0.1259]], device='cuda:0'), tensor([[-0.4238, -2.1601, -1.2181, -2.9873,  6.7331],
        [-1.0026, -2.1594, -1.4156, -0.9180,  5.5819],
        [-1.4523, -3.0208, -1.9554, -2.0613,  7.8702],
        [-1.6201,  6.6043, -1.8541, -1.9407, -2.0429],
        [-1.5791, -2.2544, -0.4790, -2.3945,  7.2269],
        [-1.6012, -1.5460, -1.4590, -1.9168,  6.9186],
        [-1.9543, -2.7834, -0.9915, -2.0100,  6.9887],
        [-0.9464, -1.8560,  6.6339, -1.2888, -1.8343],
        [-2.2600, -3.0477, -0.4618, -0.7330,  7.0548],
        [-2.6336, -3.5206, -1.3821, -1.2067,  6.9502],
        [-2.1216,  8.0958, -1.8895, -2.4062, -1.7012],
        [-1.5792, -3.1160, -2.0588, -1.9987,  7.5276],
        [ 8.5811, -2.0030, -1.8110, -0.8963, -1.0145],
        [ 8.2625, -1.8540, -0.5951, -0.6718,  0.7483],
        [-2.2815, -2.7960, -0.6698, -1.8150,  7.1744],
        [-1.7122, -1.8812, -1.5829, -1.8235,  7.8601],
        [-1.2951, -2.4561, -1.5592, -1.6668,  7.8996],
        [-1.2086, -2.5749,  6.3700, -1.2940, -0.7722],
        [ 8.1878, -1.4321, -0.0764, -1.0937, -0.5081],
        [ 4.8610, -1.4281, -0.9339, -1.1355,  0.2239],
        [ 7.0759, -2.5577, -0.1852, -0.7463, -0.0286],
        [-1.6176,  6.6058, -1.6704, -1.8067, -2.1906],
        [ 8.3711, -1.7587, -0.0814, -1.4376, -2.0958],
        [-1.8224, -2.9167, -2.2442, -2.0837,  7.8646],
        [-1.8555, -2.0692, -1.0550, -1.8496,  7.9635],
        [-1.3295, -2.2646, -1.1544, -2.5728,  6.9153],
        [-3.0687,  6.4615, -2.0648, -2.2207, -1.6171],
        [-2.0573, -1.9520,  3.2069, -1.1170,  0.8020],
        [-2.4100, -2.5737, -0.2531, -1.9385,  7.6650],
        [ 6.7983, -2.3781, -0.5620, -2.2812, -0.6819],
        [-1.7738, -2.8662, -2.2134, -1.3799,  8.0274],
        [ 8.5587, -1.4930, -0.5318, -0.7127,  0.9535]], device='cuda:0')]
(Pdb) Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 303, in <module>
    acc, ent = accuracy_ent(algorithm, loader, weights, device, adapt=None)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 96, in accuracy_ent
    p = network(x)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 96, in accuracy_ent
    p = network(x)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Base model's results
Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 312, in <module>
    acc, ent = accuracy_ent(algorithm, loader, weights, device, adapt=None)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 114, in accuracy_ent
    if p.size(1) == 1:
AttributeError: 'list' object has no attribute 'size'
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Base model's results
Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 302, in <module>
    acc, ent = accuracy_ent(algorithm, loader, weights, device, adapt=None)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 95, in accuracy_ent
    p = network(x)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 201, in _forward_unimplemented
    raise NotImplementedError
NotImplementedError
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Base model's results
Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 308, in <module>
    acc, ent = accuracy_ent(target_network, loader, weights, device, adapt=None)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 104, in accuracy_ent
    if p.size(1) == 1:
AttributeError: 'list' object has no attribute 'size'
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Base model's results
Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 308, in <module>
    acc, ent = accuracy_ent(target_network, loader, weights, device, adapt=None)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 104, in accuracy_ent
    if p.size(1) == 1:
AttributeError: 'list' object has no attribute 'size'
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Base model's results
Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 306, in <module>
    acc, ent = accuracy_ent(target_network, loader, weights, device, adapt=None)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 104, in accuracy_ent
    if p.size(1) == 1:
AttributeError: 'list' object has no attribute 'size'
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Base model's results
Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 306, in <module>
    acc, ent = accuracy_ent(target_network, loader, weights, device, adapt=None)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 104, in accuracy_ent
    if p.size(1) == 1:
AttributeError: 'list' object has no attribute 'size'
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Base model's results
> /local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py(307)<module>()
-> acc, ent = accuracy_ent(target_network, loader, weights, device, adapt=None)
(Pdb) VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=384, out_features=5, bias=True)
)
(Pdb) <class 'domainbed.visiontransformer.VisionTransformer'>
(Pdb) <class 'domainbed.algorithms.ERM_ViT'>
(Pdb) ERM_ViT(
  (student): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
    (head): Linear(in_features=384, out_features=5, bias=True)
  )
)
(Pdb) ['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_backward_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_update_ema_variables', '_version', 'add_module', 'apply', 'bfloat16', 'buffers', 'children', 'cpu', 'cuda', 'cutmix', 'double', 'dump_patches', 'ema', 'eval', 'extra_repr', 'float', 'forward', 'half', 'hparams', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'optimizer', 'parameters', 'predict', 'predict_Train', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_parameter', 'requires_grad_', 'share_memory', 'state_dict', 'student', 'to', 'train', 'training', 'type', 'update', 'xpu', 'zero_grad']
(Pdb) Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 307, in <module>
    misc.print_row([results[key] for key in results_keys], colwidth=12)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 307, in <module>
    misc.print_row([results[key] for key in results_keys], colwidth=12)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Base model's results
env0_in_acc   env0_in_ent   env0_out_acc  env0_out_ent  env1_in_acc   env1_in_ent   env1_out_acc  env1_out_ent  env2_in_acc   env2_in_ent   env2_out_acc  env2_out_ent  env3_in_acc   env3_in_ent   env3_out_acc  env3_out_ent 
1.0000000000  0.0099255635  1.0000000000  0.0175711681  0.6522352941  0.2165889695  0.6516007533  0.2383265379  0.9253617669  0.2755164540  0.8262195122  0.3420602527  0.9563124769  0.1914578277  0.8785185185  0.2497821510 

After T3A
Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 315, in <module>
    loader1, loader2, ent = generate_featurelized_loader(loader, network=algorithm.featurizer, classifier=algorithm.classifier, batch_size=32)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 947, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'ERM_ViT' object has no attribute 'featurizer'
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Base model's results
env0_in_acc   env0_in_ent   env0_out_acc  env0_out_ent  env1_in_acc   env1_in_ent   env1_out_acc  env1_out_ent  env2_in_acc   env2_in_ent   env2_out_acc  env2_out_ent  env3_in_acc   env3_in_ent   env3_out_acc  env3_out_ent 
1.0000000000  0.0099255635  1.0000000000  0.0175711681  0.6522352941  0.2165889695  0.6516007533  0.2383265379  0.9253617669  0.2755164540  0.8262195122  0.3420602527  0.9563124769  0.1914578277  0.8785185185  0.2497821510 

After T3A
Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 318, in <module>
    loader1, loader2, ent = generate_featurelized_loader(loader, network=algorithm.student.featurizer, classifier=algorithm.student.classifier, batch_size=32)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 947, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'VisionTransformer' object has no attribute 'featurizer'
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
> /local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py(296)<module>()
-> algorithm.load_state_dict(algorithm_dict)
(Pdb) ERM_ViT(
  (student): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
    (head): Linear(in_features=384, out_features=5, bias=True)
  )
)
(Pdb) VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=384, out_features=5, bias=True)
)
(Pdb) Linear(in_features=384, out_features=5, bias=True)
(Pdb) *** AttributeError: 'VisionTransformer' object has no attribute 'network'
(Pdb) --KeyboardInterrupt--
(Pdb) Sequential(
  (0): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
(Pdb) *** AttributeError: 'ERM_ViT' object has no attribute 'teacher'
(Pdb) Linear(in_features=384, out_features=5, bias=True)
(Pdb) Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 296, in <module>
    algorithm.load_state_dict(algorithm_dict)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 296, in <module>
    algorithm.load_state_dict(algorithm_dict)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: ./domainbed/VLCS/ERM_ViT_ours/ERM_ViT_ours_DeiT-small/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
> /local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py(296)<module>()
-> algorithm.load_state_dict(algorithm_dict)
(Pdb) Base model's results
env0_in_acc   env0_in_ent   env0_out_acc  env0_out_ent  env1_in_acc   env1_in_ent   env1_out_acc  env1_out_ent  env2_in_acc   env2_in_ent   env2_out_acc  env2_out_ent  env3_in_acc   env3_in_ent   env3_out_acc  env3_out_ent 
1.0000000000  0.0099255635  1.0000000000  0.0175711681  0.6522352941  0.2165889695  0.6516007533  0.2383265379  0.9253617669  0.2755164540  0.8262195122  0.3420602527  0.9563124769  0.1914578277  0.8785185185  0.2497821510 

After T3A
Traceback (most recent call last):
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 296, in <module>
    
  File "/local_ssd2/jeom/SDViT/domainbed/scripts/unsupervised_adaptation.py", line 58, in generate_featurelized_loader
    z = network(x)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/nfs/site/home/jeom/miniconda3/envs/dg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
TypeError: forward() takes 1 positional argument but 2 were given
Environment:
	Python: 3.8.13
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.23.3
	PIL: 9.2.0
Args:
	adapt_algorithm: T3A
	algorithm: ERM_ViT
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone":"DeitSmall","batch_size":32,"lr":5e-05,"resnet_dropout":0.0,"weight_decay":0.0}
	hparams_seed: 0
	input_dir: logs/VLCS/ERM-ViT/baseline/2d91c2986a7dfa724049aa26efc82023
	output_dir: logs/VLCS/ERM-ViT/baseline/2d91c2986a7dfa724049aa26efc82023
	save_best_model: True
	save_model_every_checkpoint: False
	seed: 2016627605
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_batch_size: 32
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	CutMix: False
	EMA: False
	EMA_decay: 0.999
	backbone: DeitSmall
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Base model's results
env0_in_acc   env0_in_ent   env0_out_acc  env0_out_ent  env1_in_acc   env1_in_ent   env1_out_acc  env1_out_ent  env2_in_acc   env2_in_ent   env2_out_acc  env2_out_ent  env3_in_acc   env3_in_ent   env3_out_acc  env3_out_ent 
1.0000000000  0.0099255635  1.0000000000  0.0175711681  0.6522352941  0.2165889695  0.6516007533  0.2383265379  0.9253617669  0.2755164540  0.8262195122  0.3420602527  0.9563124769  0.1914578277  0.8785185185  0.2497821510 

After T3A
env0_in_acc   env0_in_ent   env0_out_acc  env0_out_ent  env1_in_acc   env1_in_ent   env1_out_acc  env1_out_ent  env2_in_acc   env2_in_ent   env2_out_acc  env2_out_ent  env3_in_acc   env3_in_ent   env3_out_acc  env3_out_ent  filter_K     
1.0000000000  0.0000219610  1.0000000000  0.0007506328  0.5842803030  0.1361306422  0.6250000000  0.1475316458  0.8951981707  0.1523305836  0.7234375000  0.2074231148  0.9326636905  0.0946185142  0.8690476190  0.1254440766  1            
1.0000000000  0.0000113445  1.0000000000  0.0006571173  0.6396780303  0.1009877343  0.6777343750  0.1021070436  0.9134908537  0.1126012615  0.8015625000  0.1386434093  0.9427083333  0.0738167196  0.8809523810  0.1054282153  5            
1.0000000000  0.0000099997  1.0000000000  0.0003977294  0.6609848485  0.0865653007  0.6816406250  0.0818534758  0.9142530488  0.0959003236  0.8125000000  0.1175296362  0.9501488095  0.0615717566  0.8750000000  0.0834562449  20           
1.0000000000  0.0000085113  1.0000000000  0.0004029588  0.6756628788  0.0880020901  0.6835937500  0.0747454007  0.9173018293  0.0875865259  0.8234375000  0.1164079366  0.9494047619  0.0571226915  0.8809523810  0.0813714119  50           
1.0000000000  0.0000088173  1.0000000000  0.0003835819  0.6785037879  0.0844351325  0.6875000000  0.0762705499  0.9165396341  0.0875739245  0.8203125000  0.1240867030  0.9501488095  0.0574682004  0.8794642857  0.0791910353  100          
1.0000000000  0.0000083904  1.0000000000  0.0003492162  0.6742424242  0.0703652368  0.6914062500  0.0809522892  0.9115853659  0.1002702129  0.8171875000  0.1328337517  0.9505208333  0.0552848149  0.8839285714  0.0691695065  -1           
